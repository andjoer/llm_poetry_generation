{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andjoer/llm_poetry_generation/blob/main/colabs/LLM_alliterations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83YQtxlRLHyw"
   },
   "source": [
    "# Creating alliteration patterns with large language models and beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyP62vGZLI9w"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yvxh2q6YLHy1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SYp5zlU_LHy4"
   },
   "outputs": [],
   "source": [
    "model_name = \"Anjoe/german-poetry-gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jsCo6ZhOLHy5"
   },
   "outputs": [],
   "source": [
    "def create_word_start_mask(tokenizer):    \n",
    "    word_start_mask =  np.zeros(len(tokenizer))\n",
    "    for i in range(len(tokenizer)):\n",
    "        if tokenizer.decode(i)[0] == ' ':\n",
    "            word_start_mask[i] = 1\n",
    "    return word_start_mask\n",
    "        \n",
    "def perplexity(text):\n",
    "    device = model.device\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / end_loc).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bUaToIPFLHy7"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "# Modified so that it works more on a word level. \n",
    "# Example \"das Denkende\" and \"das Denken\" are the same n-gram. \n",
    "################################################################################\n",
    "\n",
    "def _get_word_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx]\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_word_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "\n",
    "    cur_len = len(prev_input_ids)\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len])\n",
    "\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_word_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, \n",
    "                                   num_hypos: int, cur_len: int, word_start_mask: np.array):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    prev_input_ids = [[item for item in prev_inputs.tolist() if word_start_mask[item]==1] \n",
    "                      for prev_inputs in prev_input_ids] # MODIFICATION\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_word_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_word_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "\n",
    "    return banned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sK4p6HwTLHy9"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "################################################################################\n",
    "\n",
    "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "    return banned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9mhzTzA3LHy_"
   },
   "outputs": [],
   "source": [
    "def find_word_beginning(token_ids):    \n",
    "    for j in range(1,len(token_ids)):\n",
    "                possible_beginning = tokenizer.decode(token_ids[-j])\n",
    "                if possible_beginning[0] == ' ' and possible_beginning.strip():\n",
    "                    return possible_beginning.strip(),j\n",
    "                    \n",
    "    else:\n",
    "        return tokenizer.decode(token_ids[0]).strip(),len(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "- block all logits that are not starting with the correct letter (except they continue an other word)\n",
    "- block all tokens that would result in too many repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZvKnn-fULHy_"
   },
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "class alit_logits(LogitsProcessor):\n",
    "    def __init__(self, tokenizer,block_token_dict, \n",
    "              word_beginnings = False,\n",
    "              ngram_size_words = 2,\n",
    "              ngram_size_tokens = 4,\n",
    "              max_word_len = 4,\n",
    "              len_rand = True,\n",
    "              randomize = False,\n",
    "              random = 6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_token_dict = block_token_dict\n",
    "        self.word_beginnings = word_beginnings\n",
    "        self.ngram_size_words = ngram_size_words\n",
    "        self.ngram_size_tokens = ngram_size_tokens\n",
    "        self.word_start_mask = create_word_start_mask(tokenizer)\n",
    "        self.max_word_len = max_word_len\n",
    "        self.len_rand = len_rand\n",
    "        self.randomize = randomize\n",
    "        self.random = random\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        banned_tokens = []\n",
    "    \n",
    "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):    \n",
    "\n",
    "            banned = []\n",
    "            \n",
    "            if len(beam_input_ids) > 0:\n",
    "                if self.word_beginnings:\n",
    "                    last_word, word_len = find_word_beginning(beam_input_ids)\n",
    "                    last_letter = last_word[0]\n",
    "                else:\n",
    "                    last_letter = self.tokenizer.decode(beam_input_ids[-1])[0]\n",
    "                    _, word_len =  find_word_beginning(beam_input_ids)\n",
    "\n",
    "            else: \n",
    "                last_letter = list(block_token_dict)[0]\n",
    "\n",
    "            last_letter = last_letter.lower()\n",
    "\n",
    "            if last_letter in list(self.block_token_dict):\n",
    "                banned = self.block_token_dict[last_letter][0]\n",
    "            else: \n",
    "                if self.word_beginnings: \n",
    "                    banned = (list(self.block_token_dict.values())[0][1])\n",
    "                else: \n",
    "                    banned = (list(self.block_token_dict.values())[0][0])\n",
    "                    \n",
    "            if self.len_rand:              \n",
    "                max_word_len = random.randint(1,self.max_word_len)\n",
    "                \n",
    "            else: \n",
    "                max_word_len = self.max_word_len\n",
    "                \n",
    "            if max_word_len: \n",
    "                if word_len >= max_word_len: \n",
    "                    banned += list(np.where(self.word_start_mask == 0)[0])  \n",
    "                    \n",
    "            banned_tokens.append(banned)\n",
    "            \n",
    "\n",
    "        num_batch_hypotheses = scores.shape[0]\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_word_tokens = _calc_banned_word_ngram_tokens(self.ngram_size_words,\n",
    "                                                            input_ids,\n",
    "                                                            num_batch_hypotheses,\n",
    "                                                            cur_len,\n",
    "                                                            self.word_start_mask)\n",
    "        \n",
    "        banned_token_tokens = _calc_banned_ngram_tokens(self.ngram_size_tokens, \n",
    "                                                        input_ids, \n",
    "                                                        num_batch_hypotheses, \n",
    "                                                        cur_len)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (len(banned_tokens)): \n",
    "            banned_tokens[i] += banned_word_tokens[i] + banned_token_tokens[i]\n",
    "\n",
    "        for i, banned_token in enumerate(banned_tokens):\n",
    "            scores[i, banned_token] = -float(\"inf\")\n",
    "            \n",
    "        ##############################################################\n",
    "        # randomize\n",
    "        if self.randomize:\n",
    "     \n",
    "            scores *= torch.randn(scores.shape)/self.random+1\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P9t5-zi-LHzB"
   },
   "outputs": [],
   "source": [
    "def check_beginnings(letter,decoded,keep_tokens = ['.'],word_beginnings = True):\n",
    "    \n",
    "    if word_beginnings:\n",
    "        word_beginning = decoded[0] == ' '\n",
    "    else:\n",
    "        word_beginning = True\n",
    "        \n",
    "    decoded = decoded.strip()\n",
    "    if decoded != '':\n",
    "        start_letter = decoded[0].lower() != letter\n",
    "    else: \n",
    "        start_letter = True\n",
    "    not_alpha =  not decoded.isalpha()\n",
    "    \n",
    "    not_keep = decoded not in keep_tokens\n",
    "    if (word_beginning and start_letter or not_alpha) and not_keep :\n",
    "        return True\n",
    "        \n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l6KjujAfLHzD"
   },
   "outputs": [],
   "source": [
    "def create_block_token_dict(tokenizer,letters,keep_tokens,word_beginnings):\n",
    "    block_token_dict = {}\n",
    "\n",
    "    for i, letter in enumerate(letters):\n",
    "        block_tokens = []\n",
    "        for j in range(len(tokenizer)):\n",
    "            decoded = tokenizer.decode(j)\n",
    "            if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = word_beginnings):\n",
    "                block_tokens.append(j)\n",
    "        block_token_dict[letters[i-1]] = [block_tokens]\n",
    "\n",
    "    if word_beginnings:\n",
    "        for i, letter in enumerate(letters):\n",
    "            block_tokens = []\n",
    "            for j in range(len(tokenizer)):\n",
    "                decoded = tokenizer.decode(j)\n",
    "                if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = False):\n",
    "                    block_tokens.append(j)\n",
    "            block_token_dict[letters[i-1]].append(block_tokens)\n",
    "            \n",
    "    return block_token_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "p1DKBu6DLHzE"
   },
   "outputs": [],
   "source": [
    "def create_aliterations(prompt, letters, tokenizer,\n",
    "                       keep_tokens = [],\n",
    "                       word_beginnings = False,\n",
    "                       max_length = 24,\n",
    "                       num_beams = 15,\n",
    "                       num_return_beams = 14,\n",
    "                       ngram_size_words = 2,\n",
    "                       ngram_size_tokens = 4,\n",
    "                       max_word_len = 4,\n",
    "                       len_rand = False,\n",
    "                       randomize = False,\n",
    "                       random = 6):\n",
    "\n",
    "    if num_beams < num_return_beams:\n",
    "        print('warning: setting number of return beams equal to number of beams')\n",
    "        num_return_beams = num_beams\n",
    "    block_token_dict = create_block_token_dict(tokenizer,letters,keep_tokens,word_beginnings)\n",
    "    prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "    prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "    beam_scorer = BeamSearchScorer(\n",
    "        batch_size = prompt_tokenized.shape[0],\n",
    "        num_beams = num_beams,\n",
    "        num_beam_hyps_to_keep = num_return_beams,\n",
    "        device=model.device\n",
    "    )\n",
    "\n",
    "    \n",
    "    logits_processor = LogitsProcessorList([alit_logits(tokenizer,\n",
    "                                                        block_token_dict,\n",
    "                                                        word_beginnings = word_beginnings,\n",
    "                                                        ngram_size_words = ngram_size_words,\n",
    "                                                        ngram_size_tokens = ngram_size_tokens,\n",
    "                                                        max_word_len = max_word_len,\n",
    "                                                        len_rand = len_rand,\n",
    "                                                        randomize = randomize,\n",
    "                                                        random = random)])\n",
    "\n",
    "    generated = model.beam_search(\n",
    "        torch.cat([prompt_tokenized] * num_beams),\n",
    "        beam_scorer,\n",
    "        logits_processor = logits_processor,\n",
    "        stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    "    )\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1TKh3s-HLHzG",
    "outputId": "2d02f2c3-0329-4c1b-bba9-5e6b25cbe715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam 0: der duftigen Auberlaune dort an dem Arm des Apollo dich auf dein Angesicht drück\n",
      "beam 1: der duftigen Auberlaune dort an dem Arm des Apollo dich auf dem Acker du\n",
      "beam 2: der duftigen Auberlaune dort an dem Arm des Apollo dich auf den Asphalt drück\n",
      "beam 3: der duftigen Auberlaune dort an dem Arm des Apollo dich auf dein Amt des\n",
      "beam 4: der duftigen Auberlaune dort an dem Arm des Apollo dich auf dein Angesicht du\n",
      "beam 5: der duftigen Auberlaune dort an dem Arm des Apollo dich auf den Ausgang des\n",
      "beam 6: der duftigen Auberlaune dort an dem Arm des Apollo dich auf den Ausgang deiner\n",
      "beam 7: der duftigen Auberlaune dort an diesem Abend dort auf dem alten Deich am Deich\n",
      "beam 8: der duftigen Auberlaune dort an dem Arm des Apollo dich auf den Ausgang drückte\n",
      "beam 9: der duftigen Auberlaune dort an dem Arm des Apollo dich auf deinem Atem durch\n"
     ]
    }
   ],
   "source": [
    "prompt = '<|endoftext|>'\n",
    "\n",
    "letters = ['d','a']\n",
    "generated = create_aliterations(prompt,letters,tokenizer,\n",
    "                                word_beginnings = True, # only the words should begin with the same character,\n",
    "                                                        # if False all tokens will begin with the same character\n",
    "                                max_length = 20,        # number of tokens after which the beam search stops\n",
    "                                num_beams = 18,         # number of beams the algorithm will try\n",
    "                                num_return_beams = 10,  # total number of beams that will be kept after each step\n",
    "                                ngram_size_words = 2,   # maximum number a word n-gram may be repeated\n",
    "                                ngram_size_tokens = 4,  # maximum number of token n-gram may be repeaded\n",
    "                                max_word_len = 4,       # maximum number of tokens a word may contain\n",
    "                                len_rand = False,       # make the maximum number of tokens for a word random \n",
    "                                randomize = True,     # additional sampling of the output\n",
    "                                random = 6)            # random value for the output (more is less random)\n",
    "\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "    output = tokenizer.decode(output_tokenized,skip_special_tokens = True)\n",
    "    print(f'beam {index}: {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VLJGgShLNyy"
   },
   "outputs": [],
   "source": [
    "%mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Kl6iX_0OLHzI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc perp\n",
      "calc perp\n",
      "calc perp\n",
      "Anjoe/german-poetry-gpt2-large\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob den o //perplexity: 241.93138\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob demselben o //perplexity: 244.20912\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob dieselben Or //perplexity: 420.29144\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob demselben Offizier //perplexity: 298.7327\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob demselben ort //perplexity: 257.03812\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob das Ord //perplexity: 516.5403\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Ort den Opfern dann o //perplexity: 360.0826\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob demselben over //perplexity: 331.87393\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben Ob das Ord //perplexity: 513.92255\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob dieselben ober //perplexity: 414.92276\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob diesem Or //perplexity: 381.04095\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben Ob die o //perplexity: 262.26126\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben Orts diesmal ohne //perplexity: 467.77252\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob diesem o //perplexity: 277.1455\n",
      "opfre deineberückend ob dem Orden des Ordens des Ortes dasselbe oder derselbe oder dasselbe Orden dieselbe Orden den Orten desselben ob den Orts //perplexity: 363.43097\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August e alt //perplexity: 352.21426\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Einen Abend //perplexity: 365.8096\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August em Anfang //perplexity: 328.9573\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Eid al //perplexity: 637.17\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am Ende and einem af en alte //perplexity: 324.3596\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Einen Ab //perplexity: 687.9415\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August em Ausland //perplexity: 379.1515\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August erwärmt auf //perplexity: 454.40332\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Einen Augenblick //perplexity: 375.72833\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Erlaubnis aus //perplexity: 481.22473\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August Einen Affen //perplexity: 512.8885\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August e and //perplexity: 492.36307\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August erwärmt am //perplexity: 497.17065\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am Ende and einem af en ann //perplexity: 515.58203\n",
      "es Ereigniß an einer alten Eiche auf einem ac Empfang am ernste Abende Ende August em Amtsgerichts //perplexity: 561.07764\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erster eigener eigener erster //perplexity: 178.06743\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erster eigener eigener Eindruck //perplexity: 166.27986\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erster eigener erster //perplexity: 198.84901\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erster eigener eigenen erster //perplexity: 209.59961\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erster eigener erste //perplexity: 236.66739\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erster eigener eigener erf //perplexity: 354.68738\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erstes eigener eigener erster //perplexity: 187.53024\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erster eigener einiger //perplexity: 264.26578\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erleben erstes eig //perplexity: 359.6874\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erster eigener echter //perplexity: 192.33107\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erstes eigenen erstes erbaut //perplexity: 285.7164\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes echtes echtes erstes echte //perplexity: 198.89908\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erster eigener eigener enge //perplexity: 232.0356\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes eigenes erster eigener erf //perplexity: 371.46503\n",
      "erglänzt ein eben erst ent erotische erstes eigene erstes eigenes erstes eigenem erstes eig //perplexity: 264.80377\n",
      "\n",
      "*** final output ***\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logs/poem_1.log'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1254e25b5775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0msave_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'***'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logs/poem_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.log'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                      \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logs/poem_1.log'"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "files = glob.glob(\"logs/*.log\")\n",
    "max_idx = 0\n",
    "for file in files: \n",
    "    max_idx = max(int(re.findall(r'\\d+', file)[0]),max_idx)\n",
    "\n",
    "start_idx = max_idx + 1\n",
    "\n",
    "#letter_selection = ['a','b','d','e','f','g','h','i','j','k','l','m','n','o','s','t','u']\n",
    "\n",
    "max_num_letters = 2\n",
    "letter_selection = [[]]*max_num_letters\n",
    "letter_selection[0] = ['a','d','e','i','o','s','u']\n",
    "if max_num_letters > 1:\n",
    "    letter_selection[1] = letter_selection[0]        # option to use different sets for different positions\n",
    "\n",
    "prompt = '<|endoftext|>'\n",
    "\n",
    "\n",
    "num_beams_range = [15,30]\n",
    "num_return_beams = 15\n",
    "max_length_range = [12,35]\n",
    "\n",
    "\n",
    "for iteration in range(5):\n",
    "    save_text = model_name+'\\n'\n",
    "    final_output = []\n",
    "    for _ in range(3):\n",
    "        num_letters = random.randint(1,max_num_letters)\n",
    "        letters = []\n",
    "        for i in range(num_letters):\n",
    "            letters.append(random.choice(letter_selection[i]))\n",
    "\n",
    "        num_beams = random.randint(*num_beams_range)\n",
    "        max_length = random.randint(*max_length_range)\n",
    " \n",
    "        generated = create_aliterations(prompt,letters,tokenizer,\n",
    "                                word_beginnings = True,                           \n",
    "                                max_length = max_length,        \n",
    "                                num_beams = num_beams,        \n",
    "                                num_return_beams = num_return_beams,  \n",
    "                                ngram_size_words = 2,   \n",
    "                                ngram_size_tokens = 4,  \n",
    "                                max_word_len = 4,      \n",
    "                                len_rand = False,\n",
    "                                randomize = True,\n",
    "                                random = 6)       \n",
    "\n",
    "        perplexities = []\n",
    "        output_tmp = []\n",
    "\n",
    "        for j, output_tokenized in enumerate(generated):\n",
    "            output = tokenizer.decode(output_tokenized,skip_special_tokens = True).strip()\n",
    "            perpl = perplexity(output+'.')\n",
    "            perplexities.append(perpl)\n",
    "            save_text += output +' //perplexity: ' + str(perpl) +'\\n' \n",
    "            output_tmp.append(output)\n",
    "\n",
    "        if min(perplexities) < 400:\n",
    "            choice_idx = np.argmin(np.asarray(perplexities))\n",
    "            final_output.append(output_tmp[choice_idx])\n",
    "\n",
    "    if final_output:\n",
    "        save_text += '\\n*** final output ***\\n'\n",
    "        for output in final_output:\n",
    "            save_text += output + '\\n\\n'\n",
    "\n",
    "        save_text += '***'\n",
    "\n",
    "        with open('logs/poem_' + str(iteration+start_idx)+'.log', 'w') as f:\n",
    "                     f.write(save_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKNzohDlLHzL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LLM_alliterations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
