{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andjoer/llm_poetry_generation/blob/main/colabs/LLM_alliterations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83YQtxlRLHyw"
      },
      "source": [
        "# Creating alliteration patterns with large language models and beam search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "zyP62vGZLI9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvxh2q6YLHy1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYp5zlU_LHy4"
      },
      "outputs": [],
      "source": [
        "model_name = \"Anjoe/german-poetry-gpt2-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsCo6ZhOLHy5"
      },
      "outputs": [],
      "source": [
        "def create_word_start_mask(tokenizer):    \n",
        "    word_start_mask =  np.zeros(len(tokenizer))\n",
        "    for i in range(len(tokenizer)):\n",
        "        if tokenizer.decode(i)[0] == ' ':\n",
        "            word_start_mask[i] = 1\n",
        "    return word_start_mask\n",
        "        \n",
        "def perplexity(text):\n",
        "    device = model.device\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
        "    import torch\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    max_length = model.config.n_positions\n",
        "    stride = 512\n",
        "\n",
        "    nlls = []\n",
        "    for i in range(0, encodings.input_ids.size(1), stride):\n",
        "        begin_loc = max(i + stride - max_length, 0)\n",
        "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "        trg_len = end_loc - i  # may be different from stride on last loop\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).sum() / end_loc).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUaToIPFLHy7"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
        "# Modified so that it works more on a word level. \n",
        "# Example \"das Denkende\" and \"das Denken\" are the same n-gram. \n",
        "################################################################################\n",
        "\n",
        "def _get_word_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
        "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
        "    for idx in range(num_hypos):\n",
        "        gen_tokens = prev_input_ids[idx]\n",
        "        generated_ngram = generated_ngrams[idx]\n",
        "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
        "\n",
        "            prev_ngram_tuple = tuple(ngram[:-1])\n",
        "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
        "\n",
        "    return generated_ngrams\n",
        "\n",
        "\n",
        "def _get_generated_word_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
        "\n",
        "    cur_len = len(prev_input_ids)\n",
        "    start_idx = cur_len + 1 - ngram_size\n",
        "\n",
        "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len])\n",
        "\n",
        "    return banned_ngrams.get(ngram_idx, [])\n",
        "\n",
        "\n",
        "def _calc_banned_word_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, \n",
        "                                   num_hypos: int, cur_len: int, word_start_mask: np.array):\n",
        "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
        "    prev_input_ids = [[item for item in prev_inputs.tolist() if word_start_mask[item]==1] \n",
        "                      for prev_inputs in prev_input_ids] # MODIFICATION\n",
        "    if cur_len + 1 < ngram_size:\n",
        "        return [[] for _ in range(num_hypos)]\n",
        "\n",
        "    generated_ngrams = _get_word_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
        "\n",
        "\n",
        "    banned_tokens = [\n",
        "        _get_generated_word_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
        "        for hypo_idx in range(num_hypos)\n",
        "    ]\n",
        "\n",
        "    return banned_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sK4p6HwTLHy9"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
        "################################################################################\n",
        "\n",
        "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
        "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
        "    for idx in range(num_hypos):\n",
        "        gen_tokens = prev_input_ids[idx].tolist()\n",
        "        generated_ngram = generated_ngrams[idx]\n",
        "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
        "            prev_ngram_tuple = tuple(ngram[:-1])\n",
        "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
        "    return generated_ngrams\n",
        "\n",
        "\n",
        "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
        "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
        "    start_idx = cur_len + 1 - ngram_size\n",
        "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
        "    return banned_ngrams.get(ngram_idx, [])\n",
        "\n",
        "\n",
        "def _calc_banned_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int):\n",
        "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
        "    if cur_len + 1 < ngram_size:\n",
        "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
        "        return [[] for _ in range(num_hypos)]\n",
        "\n",
        "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
        "\n",
        "    banned_tokens = [\n",
        "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
        "        for hypo_idx in range(num_hypos)\n",
        "    ]\n",
        "    return banned_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mhzTzA3LHy_"
      },
      "outputs": [],
      "source": [
        "def find_word_beginning(token_ids):    \n",
        "    for j in range(1,len(token_ids)):\n",
        "                possible_beginning = tokenizer.decode(token_ids[-j])\n",
        "                if possible_beginning[0] == ' ' and possible_beginning.strip():\n",
        "                    return possible_beginning.strip(),j\n",
        "                    \n",
        "    else:\n",
        "        return tokenizer.decode(token_ids[0]).strip(),len(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvKnn-fULHy_"
      },
      "outputs": [],
      "source": [
        "from transformers import LogitsProcessor\n",
        "import numpy as np\n",
        "\n",
        "class alit_logits(LogitsProcessor):\n",
        "    def __init__(self, tokenizer,block_token_dict, \n",
        "              word_beginnings = False,\n",
        "              ngram_size_words = 2,\n",
        "              ngram_size_tokens = 4,\n",
        "              max_word_len = 4,\n",
        "              len_rand = True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_token_dict = block_token_dict\n",
        "        self.word_beginnings = word_beginnings\n",
        "        self.ngram_size_words = ngram_size_words\n",
        "        self.ngram_size_tokens = ngram_size_tokens\n",
        "        self.word_start_mask = create_word_start_mask(tokenizer)\n",
        "        self.max_word_len = max_word_len\n",
        "        self.len_rand = len_rand\n",
        "    \n",
        "    def __call__(self, input_ids, scores):\n",
        "        \n",
        "        banned_tokens = []\n",
        "    \n",
        "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):    \n",
        "\n",
        "            banned = []\n",
        "            \n",
        "            if len(beam_input_ids) > 0:\n",
        "                if self.word_beginnings:\n",
        "                    last_word, word_len = find_word_beginning(beam_input_ids)\n",
        "                    last_letter = last_word[0]\n",
        "                else:\n",
        "                    last_letter = self.tokenizer.decode(beam_input_ids[-1])[0]\n",
        "                    _, word_len =  find_word_beginning(beam_input_ids)\n",
        "\n",
        "            else: \n",
        "                last_letter = list(block_token_dict)[0]\n",
        "\n",
        "            last_letter = last_letter.lower()\n",
        "\n",
        "            if last_letter in list(self.block_token_dict):\n",
        "                banned = self.block_token_dict[last_letter][0]\n",
        "            else: \n",
        "                if self.word_beginnings: \n",
        "                    banned = (list(self.block_token_dict.values())[0][1])\n",
        "                else: \n",
        "                    banned = (list(self.block_token_dict.values())[0][0])\n",
        "                    \n",
        "            if self.len_rand:              \n",
        "                max_word_len = random.randint(1,self.max_word_len)\n",
        "                \n",
        "            else: \n",
        "                max_word_len = self.max_word_len\n",
        "                \n",
        "            if max_word_len: \n",
        "                if word_len >= max_word_len: \n",
        "                    banned += list(np.where(self.word_start_mask == 0)[0])  \n",
        "                    \n",
        "            banned_tokens.append(banned)\n",
        "            \n",
        "\n",
        "        num_batch_hypotheses = scores.shape[0]\n",
        "        cur_len = input_ids.shape[-1]\n",
        "        banned_word_tokens = _calc_banned_word_ngram_tokens(self.ngram_size_words,\n",
        "                                                            input_ids,\n",
        "                                                            num_batch_hypotheses,\n",
        "                                                            cur_len,\n",
        "                                                            self.word_start_mask)\n",
        "        \n",
        "        banned_token_tokens = _calc_banned_ngram_tokens(self.ngram_size_tokens, \n",
        "                                                        input_ids, \n",
        "                                                        num_batch_hypotheses, \n",
        "                                                        cur_len)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        for i in range (len(banned_tokens)): \n",
        "            banned_tokens[i] += banned_word_tokens[i] + banned_token_tokens[i]\n",
        "\n",
        "        for i, banned_token in enumerate(banned_tokens):\n",
        "            scores[i, banned_token] = -float(\"inf\")\n",
        "\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9t5-zi-LHzB"
      },
      "outputs": [],
      "source": [
        "def check_beginnings(letter,decoded,keep_tokens = ['.'],word_beginnings = True):\n",
        "    \n",
        "    if word_beginnings:\n",
        "        word_beginning = decoded[0] == ' '\n",
        "    else:\n",
        "        word_beginning = True\n",
        "        \n",
        "    decoded = decoded.strip()\n",
        "    if decoded != '':\n",
        "        start_letter = decoded[0].lower() != letter\n",
        "    else: \n",
        "        start_letter = True\n",
        "    not_alpha =  not decoded.isalpha()\n",
        "    \n",
        "    not_keep = decoded not in keep_tokens\n",
        "    if (word_beginning and start_letter or not_alpha) and not_keep :\n",
        "        return True\n",
        "        \n",
        "    else: \n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6KjujAfLHzD"
      },
      "outputs": [],
      "source": [
        "def create_block_token_dict(tokenizer,letters,keep_tokens,word_beginnings):\n",
        "    block_token_dict = {}\n",
        "\n",
        "    for i, letter in enumerate(letters):\n",
        "        block_tokens = []\n",
        "        for j in range(len(tokenizer)):\n",
        "            decoded = tokenizer.decode(j)\n",
        "            if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = word_beginnings):\n",
        "                block_tokens.append(j)\n",
        "        block_token_dict[letters[i-1]] = [block_tokens]\n",
        "\n",
        "    if word_beginnings:\n",
        "        for i, letter in enumerate(letters):\n",
        "            block_tokens = []\n",
        "            for j in range(len(tokenizer)):\n",
        "                decoded = tokenizer.decode(j)\n",
        "                if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = False):\n",
        "                    block_tokens.append(j)\n",
        "            block_token_dict[letters[i-1]].append(block_tokens)\n",
        "            \n",
        "    return block_token_dict\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1DKBu6DLHzE"
      },
      "outputs": [],
      "source": [
        "def create_aliterations(prompt, letters, tokenizer,\n",
        "                       keep_tokens = [],\n",
        "                       word_beginnings = False,\n",
        "                       max_length = 24,\n",
        "                       num_beams = 15,\n",
        "                       num_return_beams = 14,\n",
        "                       ngram_size_words = 2,\n",
        "                       ngram_size_tokens = 4,\n",
        "                       max_word_len = 4,\n",
        "                       len_rand = False\n",
        "                       ):\n",
        "\n",
        "    if num_beams < num_return_beams:\n",
        "        print('warning: setting number of return beams equal to number of beams')\n",
        "        num_return_beams = num_beams\n",
        "    block_token_dict = create_block_token_dict(tokenizer,letters,keep_tokens,word_beginnings)\n",
        "    prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
        "    prompt_tokenized = prompt_tokenized['input_ids']\n",
        "\n",
        "    beam_scorer = BeamSearchScorer(\n",
        "        batch_size = prompt_tokenized.shape[0],\n",
        "        num_beams = num_beams,\n",
        "        num_beam_hyps_to_keep = num_return_beams,\n",
        "        device=model.device\n",
        "    )\n",
        "\n",
        "    \n",
        "    logits_processor = LogitsProcessorList([alit_logits(tokenizer,\n",
        "                                                        block_token_dict,\n",
        "                                                        word_beginnings = word_beginnings,\n",
        "                                                        ngram_size_words = ngram_size_words,\n",
        "                                                        ngram_size_tokens = ngram_size_tokens,\n",
        "                                                        max_word_len = max_word_len,\n",
        "                                                        len_rand = len_rand)])\n",
        "\n",
        "    generated = model.beam_search(\n",
        "        torch.cat([prompt_tokenized] * num_beams),\n",
        "        beam_scorer,\n",
        "        logits_processor = logits_processor,\n",
        "        stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
        "    )\n",
        "    return generated\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TKh3s-HLHzG",
        "outputId": "2d02f2c3-0329-4c1b-bba9-5e6b25cbe715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "beam 0:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfern Anger Dörfer Anger\n",
            "beam 1:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger de Anger da Anger Der Anger dient als\n",
            "beam 2:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger de Anger da Anger Der Anger diente als\n",
            "beam 3:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten deutschen Adel deutscher Adels\n",
            "beam 4:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfern Anger daran Anger\n",
            "beam 5:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den Anger dieses alte deutsche Anger\n",
            "beam 6:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfern Anger dran Anger\n",
            "beam 7:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger de Anger da Anger dr Anger dar a\n",
            "beam 8:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den Anger dieses alte deutsche Adels\n",
            "beam 9:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfer Anger dann ab\n",
            "beam 10:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfern Anger dann Anger\n",
            "beam 11:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger de Anger da Anger Der Anger dient auch\n",
            "beam 12:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten deutschen Adel deutscher Art\n",
            "beam 13:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten deutschen Adel deutscher Adel\n",
            "beam 14:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfern Anger dann ab\n",
            "beam 15:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten deutschen Adel deutscher Ad\n",
            "beam 16:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten Dörfer Anger diese alte\n",
            "beam 17:  dämmernd aus der Abendröte des Abends des achten Dorfes an dem alten Dorf Anger dort an den alten deutschen Adel deutscher Aus\n"
          ]
        }
      ],
      "source": [
        "prompt = '<|endoftext|>'\n",
        "\n",
        "letters = ['d','a']\n",
        "generated = create_aliterations(prompt,letters,tokenizer,\n",
        "                                word_beginnings = True, # only the words should begin with the same character,\n",
        "                                                        # if False all tokens will begin with the same character\n",
        "                                max_length = 28,        # number of tokens after which the beam search stops\n",
        "                                num_beams = 30,         # number of beams the algorithm will try\n",
        "                                num_return_beams = 18,  # total number of beams that will be kept after each step\n",
        "                                ngram_size_words = 2,   # maximum number a word n-gram may be repeated\n",
        "                                ngram_size_tokens = 4,  # maximum number of token n-gram may be repeaded\n",
        "                                max_word_len = 4,       # maximum number of tokens a word may contain\n",
        "                                len_rand = False)       # make the maximum number of tokens for a word random \n",
        "                                                        # at each step\n",
        "\n",
        "for index, output_tokenized in enumerate(generated):\n",
        "  output = tokenizer.decode(output_tokenized,skip_special_tokens = True)\n",
        "  print(f'beam {index}: {output}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir logs"
      ],
      "metadata": {
        "id": "4VLJGgShLNyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kl6iX_0OLHzI"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import re\n",
        "\n",
        "files = glob.glob(\"logs/*.log\")\n",
        "max_idx = 0\n",
        "for file in files: \n",
        "    max_idx = max(int(re.findall(r'\\d+', file)[0]),max_idx)\n",
        "\n",
        "start_idx = max_idx + 1\n",
        "\n",
        "#letter_selection = ['a','b','d','e','f','g','h','i','j','k','l','m','n','o','s','t','u']\n",
        "\n",
        "max_num_letters = 2\n",
        "letter_selection = [[]]*max_num_letters\n",
        "letter_selection[0] = ['a','d','e','i','o','s','u']\n",
        "if max_num_letters > 1:\n",
        "    letter_selection[1] = letter_selection[0]        # option to use different sets for different positions\n",
        "\n",
        "prompt = '<|endoftext|>'\n",
        "\n",
        "\n",
        "num_beams_range = [15,30]\n",
        "num_return_beams = 15\n",
        "max_length_range = [12,35]\n",
        "\n",
        "\n",
        "for iteration in range(5):\n",
        "    save_text = model_name+'\\n'\n",
        "    final_output = []\n",
        "    for _ in range(3):\n",
        "        num_letters = random.randint(1,max_num_letters)\n",
        "        letters = []\n",
        "        for i in range(num_letters):\n",
        "            letters.append(random.choice(letter_selection[i]))\n",
        "\n",
        "        num_beams = random.randint(*num_beams_range)\n",
        "        max_length = random.randint(*max_length_range)\n",
        " \n",
        "        generated = create_aliterations(prompt,letters,tokenizer,\n",
        "                                word_beginnings = True,                           \n",
        "                                max_length = max_length,        \n",
        "                                num_beams = num_beams,        \n",
        "                                num_return_beams = num_return_beams,  \n",
        "                                ngram_size_words = 2,   \n",
        "                                ngram_size_tokens = 4,  \n",
        "                                max_word_len = 4,      \n",
        "                                len_rand = False)       \n",
        "\n",
        "        perplexities = []\n",
        "        output_tmp = []\n",
        "\n",
        "        print('calc perp')\n",
        "        for j, output_tokenized in enumerate(generated):\n",
        "            output = tokenizer.decode(output_tokenized,skip_special_tokens = True).strip()\n",
        "            perpl = perplexity(output+'.')\n",
        "            perplexities.append(perpl)\n",
        "            save_text += output +' //perplexity: ' + str(perpl) +'\\n' \n",
        "            output_tmp.append(output)\n",
        "\n",
        "        if min(perplexities) < 400:\n",
        "            choice_idx = np.argmin(np.asarray(perplexities))\n",
        "            final_output.append(output_tmp[choice_idx])\n",
        "\n",
        "    if final_output:\n",
        "        save_text += '\\n*** final output ***\\n'\n",
        "\n",
        "        for output in final_output:\n",
        "            save_text += output + '\\n\\n'\n",
        "\n",
        "        save_text += '***'\n",
        "\n",
        "        with open('logs/poem_' + str(iteration+start_idx)+'.log', 'w') as f:\n",
        "                     f.write(save_text)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKNzohDlLHzL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "LLM_alliterations.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}