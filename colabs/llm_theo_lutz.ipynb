{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andjoer/llm_poetry_generation/blob/main/colabs/llm_theo_lutz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrZ2myE2M3NO"
      },
      "source": [
        "# Inspired by Theo Lutz\n",
        "\n",
        "## Combining large language models and patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91QWM1NGy9OK"
      },
      "source": [
        "click the 'Run all' button\n",
        "\n",
        "<img src = 'https://github.com/andjoer/llm_poetry_generation/blob/main/graphics/colab_en.jpg?raw=true'>\n",
        "\n",
        "German\n",
        "\n",
        "\n",
        "\n",
        "<img src = 'https://github.com/andjoer/llm_poetry_generation/blob/main/graphics/colab.jpg?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wXHQY89M4Z4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dkFCIitvM3NQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/andreas/.conda/envs/tf36/lib/python3.6/site-packages/packaging/requirements.py:66: UserWarning: warn_ungrouped_named_tokens_in_collection: setting results name 'specifier' on And expression collides with '_original_end' on contained expression\n",
            "  VERSION_SPEC = originalTextFor(_VERSION_SPEC)(\"specifier\")\n",
            "/home/andreas/.conda/envs/tf36/lib/python3.6/site-packages/packaging/requirements.py:69: UserWarning: warn_ungrouped_named_tokens_in_collection: setting results name 'marker' on And expression collides with '_original_end' on contained expression\n",
            "  MARKER_EXPR = originalTextFor(MARKER_EXPR())(\"marker\")\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer,GPT2LMHeadModel, pipeline\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "import functools\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ElLwUBMRNOAU"
      },
      "outputs": [],
      "source": [
        "#!python -m spacy download \"de_core_news_lg\"\n",
        "\n",
        "nlp = spacy.load(\"de_core_news_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3rENvHMM3NT"
      },
      "source": [
        "## Defining the model to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AsVgvkYIM3NU"
      },
      "outputs": [],
      "source": [
        "gpt2_model = \"Anjoe/kant-gpt2-large\"\n",
        "\n",
        "generator = pipeline('text-generation', model=gpt2_model,\n",
        "                 tokenizer=gpt2_model, framework = 'pt')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "model = GPT2LMHeadModel.from_pretrained(gpt2_model,pad_token_id = tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9GEKYuevM3NW"
      },
      "outputs": [],
      "source": [
        "def gpt2_generate(input_text,max_length= 5, num_return_sequences=30):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "    max_length += tokenizer.encode(input_text,return_tensors='pt').size(1)\n",
        "    generated = generator(input_text, max_length=max_length,return_full_text = False, num_return_sequences=num_return_sequences)\n",
        "    \n",
        "    return [item['generated_text'] for item in generated]\n",
        "\n",
        "\n",
        "def gpt2_top_k(input_text,max_length = 10,num_return_sequences=30):\n",
        "    #tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "    #model = GPT2LMHeadModel.from_pretrained(gpt2_model,pad_token_id = tokenizer.eos_token_id)\n",
        "    input_ids = tokenizer.encode(input_text,return_tensors='pt')\n",
        "    max_length += input_ids.size(1)\n",
        "    start = input_ids.size()[1]\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample = True,\n",
        "        max_length = max_length,\n",
        "        top_k = num_return_sequences,\n",
        "        num_return_sequences = num_return_sequences,\n",
        "        early_stopping = True,\n",
        "        num_repeat_ngram_size = 2\n",
        "    )\n",
        "\n",
        "    return [tokenizer.decode(sample_output[start:],skip_special_tokens=True) for sample_output in output]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_last(possible_tokens, possible_logits,tokenizer,max_word_count):\n",
        "    \n",
        "    first = True\n",
        "    while len(re.sub(r'[^A-Za-zÄÖÜäöüß ]', ' ',tokenizer.decode([tokens[-1]for tokens in possible_tokens])).split()) > max_word_count or first and possible_tokens:  # necessary when words consisting out of many tokens get shortened\n",
        "        possible_tokens[-1] = possible_tokens[-1][:-1]\n",
        "        possible_logits[-1] = possible_logits[-1][:-1]\n",
        "\n",
        "        first = False\n",
        "\n",
        "        while not possible_tokens[-1] and possible_tokens[0]:\n",
        "            possible_tokens = possible_tokens[:-1]\n",
        "            possible_tokens[-1] = possible_tokens[-1][:-1]\n",
        "\n",
        "            possible_logits = possible_logits[:-1]\n",
        "            possible_logits[-1] = possible_logits[-1][:-1]\n",
        "\n",
        "        if not possible_tokens[0]:\n",
        "            break\n",
        "\n",
        "        \n",
        "\n",
        "    return possible_tokens, possible_logits\n",
        "\n",
        "\n",
        "def gpt_sample_systematic(input,check, pattern,num_return_sequences = 1,loop_limit = 15000, top_p = None,top_k = 1000,top_k_0 = 0, temperature = 0.9,random_first = True, random_all = False, block_non_alpha = True,\n",
        "                        trunkate_after = 100,repetition_penalty=1.2):\n",
        "\n",
        "    '''\n",
        "    builds a stack of possible tokens and filtered by a specific top_p values and goes through all of them\n",
        "    \n",
        "    '''\n",
        "    input = re.sub(r'\\s([,.!?;:](?:\\s|$))', r'\\1', input.strip())\n",
        " \n",
        "    if not top_p:\n",
        "        top_p = 0.5\n",
        "\n",
        "    \n",
        "  \n",
        "    sm = torch.nn.Softmax(dim = 1)\n",
        "\n",
        "    possible_tokens = []\n",
        "    possible_logits = []\n",
        "\n",
        "    possible_combinations = []\n",
        "    combination_logits = []\n",
        "\n",
        "    max_word_count = 3\n",
        "\n",
        "    inputs = tokenizer(input,return_tensors='pt')['input_ids']\n",
        "\n",
        "    max_token_count = 5  \n",
        "\n",
        "    fulfill_pos = False\n",
        "\n",
        "    possible_end = False\n",
        "    pos_match_end = False\n",
        "    depth_lst = []\n",
        "\n",
        "    last_word_start = 0   # stop building endless words\n",
        "\n",
        "    condition_length = []\n",
        "    for condition in pattern.values():\n",
        "        if type(condition) == list:\n",
        "            condition_length.append([len(cond) for cond in condition if type(cond) == list])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(loop_limit): \n",
        "   \n",
        "            if len(possible_tokens) > 0:\n",
        "                \n",
        "\n",
        "                while not possible_tokens[-1]:\n",
        "                    possible_tokens = possible_tokens[:-1]\n",
        "                    possible_logits = possible_logits[:-1]\n",
        "\n",
        "                    if not possible_tokens:\n",
        "                        break\n",
        "\n",
        "                if not possible_tokens:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    new_tokens =  torch.reshape(torch.IntTensor([tokens[-1] for tokens in possible_tokens]),(1,-1))\n",
        "              \n",
        "                except: \n",
        "\n",
        "                    print('possible tokens')\n",
        "                    print(possible_tokens)\n",
        "                    raise Exception\n",
        "                input_tokens = torch.cat((inputs,new_tokens),1)\n",
        "\n",
        "            else: \n",
        "                input_tokens = inputs\n",
        "\n",
        "            depth_lst.append(len(possible_tokens))\n",
        "         \n",
        "            outputs = model(input_tokens)\n",
        "\n",
        "            logits = outputs.logits[:,-1,:]/temperature\n",
        "\n",
        "            last_token_test = 'test' + tokenizer.decode(torch.argmax(logits))                                   # check if next token contains a space\n",
        "      \n",
        "            gen_part = tokenizer.decode([tokens[-1]for tokens in possible_tokens])\n",
        "            generated = ' '.join(re.sub(r'[^A-Za-zÄÖÜäöüß ]', ' ',gen_part ).strip().split())\n",
        "\n",
        "\n",
        "            if generated and (len(last_token_test.split()) > 1 or not tokenizer.decode(torch.argmax(logits)).isalpha()):\n",
        "\n",
        "                fulfill_requirements = True\n",
        "                possible_end = True\n",
        "                last_word_start = len(possible_tokens)\n",
        "                generated_doc = nlp(input + ' ' + generated.strip())[-len(generated.strip().split()):]\n",
        "\n",
        "                if len(generated.split()[-1]) < 2:\n",
        "                    fulfill_requirements = False\n",
        "\n",
        "                check_val, pattern_idx = check(generated_doc)\n",
        "                if not check_val:\n",
        "           \n",
        "                    fulfill_requirements = False\n",
        "                    possible_end = False\n",
        "\n",
        "                if pattern_idx > -1:\n",
        "                    if max([cond[pattern_idx] for cond in condition_length if len(cond) > pattern_idx]) > len(generated_doc):  # this is  workaround since this sampling method was implemented later\n",
        "                        possible_end = False\n",
        "           \n",
        "           \n",
        "            else:\n",
        "                if len(possible_tokens) - last_word_start < 5:          # no endless compund tokens without space separation\n",
        "                    fulfill_requirements = True\n",
        "                else: \n",
        "                    fulfill_requirements = False\n",
        "                possible_end = False\n",
        "\n",
        "\n",
        "            if ' '.join(re.sub(r'[^A-Za-zÄÖÜäöüß ]', ' ',gen_part ).strip().split()) != ' '.join(gen_part.strip().split()): \n",
        " \n",
        "                fulfill_requirements = False\n",
        "    \n",
        "            if fulfill_requirements:\n",
        "\n",
        "                logits_sorted,indices_sorted = torch.sort(logits, descending=True)\n",
        "                logits_sorted = sm(logits_sorted)\n",
        "                cum_sum = torch.cumsum(logits_sorted, dim=-1)\n",
        "                cum_sum[:,0] = 0                  \n",
        "                                              \n",
        "                token_inside_top_p = cum_sum <= top_p                                   # keep at least one index\n",
        "\n",
        "\n",
        "            if len(possible_tokens) >= max_token_count or not fulfill_requirements:  \n",
        "            \n",
        "                if len(depth_lst) > trunkate_after*2:                                     # too many repetitions with same trunk -> the trunk could be the problem\n",
        "                    possible_tokens = possible_tokens[:1]\n",
        "                    possible_logits = possible_logits[:1]\n",
        "                    depth_lst = []\n",
        "\n",
        "                elif len(depth_lst) > trunkate_after:\n",
        "                    cut = max(int(min(depth_lst[-(trunkate_after+int(trunkate_after*0.8)):])/2),1)\n",
        "                    if cut == 1:\n",
        "                        depth_lst = []\n",
        "                    possible_tokens = possible_tokens[:cut]\n",
        "                    possible_logits = possible_logits[:cut]\n",
        "\n",
        "                possible_tokens, possible_logits = remove_last(possible_tokens, possible_logits,tokenizer,max_word_count)\n",
        "                if len(possible_tokens) == 1:\n",
        "                    depth_lst = []\n",
        "                                \n",
        "            elif fulfill_requirements and possible_end:\n",
        "                #print(tokenizer.decode([tokens[-1].item() for tokens in possible_tokens]))\n",
        "                '''print('rythm in generation function')\n",
        "                print(generated_verse.text)\n",
        "                print(generated_verse.rythm)\n",
        "                print(generated_verse.token_pos)'''\n",
        "\n",
        "                depth_lst = []\n",
        "\n",
        "                possible_combinations.append([tokens[-1].item() for tokens in possible_tokens])\n",
        "     \n",
        "                last_logits_sum = sum([logits[-1].item() for logits in possible_logits])\n",
        "                combination_logits.append(last_logits_sum)\n",
        "\n",
        "                possible_tokens, possible_logits = remove_last(possible_tokens, possible_logits,tokenizer,max_word_count)\n",
        "                \n",
        "                if not possible_tokens[0] or len(possible_combinations) >= num_return_sequences:\n",
        "                    break\n",
        "\n",
        "            elif fulfill_requirements:\n",
        "\n",
        "                if len(possible_tokens) == 0 and (len(indices_sorted[token_inside_top_p])  < top_k or top_k_0 > 0):\n",
        "\n",
        "                    indices_filtered = torch.flip(indices_sorted[0,top_k_0:top_k],dims=[-1])    # highest probability last so it gets accessed first\n",
        "                    logits_filtered = torch.flip(logits_sorted[0,top_k_0:top_k],dims=[-1])           \n",
        "            \n",
        "                else:\n",
        "                    indices_filtered = torch.flip(indices_sorted[token_inside_top_p],dims=[-1])    # highest probability last so it gets accessed first\n",
        "                    logits_filtered = torch.flip(logits_sorted[token_inside_top_p],dims=[-1])           \n",
        "\n",
        "                if random_all or (random_first and not possible_tokens):            # without randomness always the same poem would be created from the same prompt\n",
        "                    all_indices_ran = torch.multinomial(logits_filtered,num_samples = len(logits_filtered))\n",
        "                    logits_filtered = logits_filtered[all_indices_ran]\n",
        "                    indices_filtered = indices_filtered[all_indices_ran]\n",
        "                \n",
        "\n",
        "                possible_tokens.append(list(indices_filtered))\n",
        "                possible_logits.append(list(logits_filtered))\n",
        "            else:\n",
        "                possible_tokens, possible_logits = remove_last(possible_tokens, possible_logits,tokenizer,max_word_count)\n",
        "\n",
        "            if not possible_tokens:\n",
        "                break\n",
        "\n",
        "            if not possible_tokens[0]:\n",
        "                break\n",
        "\n",
        "\n",
        "    return [nlp(tokenizer.decode(combination)) for combination in possible_combinations]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4JZco4bM3NX"
      },
      "source": [
        "## Define the patterns\n",
        "\n",
        "A sequence of \"nodes\" is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r_1VKGOh-8jc"
      },
      "outputs": [],
      "source": [
        "patterns_lutz_org = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_lutz_org.append({'type':'insertion',\n",
        "                 'text':['eine','jede','keine','nicht jede','ein','jeder','kein','nicht jeder','ein','jedes','kein','nicht jedes'],  \n",
        "                 'next':1})\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_lutz_org.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'dependent':[[['gender']]],\n",
        "                 'dependency': -1,\n",
        "                 'any': False,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'number': [['Sing']],\n",
        "                 'case':[['Nom']],         # the Noun should be nominative\n",
        "                 'next':2})                           # next node\n",
        "\n",
        "            \n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_lutz_org.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':[''],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 '':['ist'],        # use when the last node is Plural\n",
        "                 'next':3})                            \n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_lutz_org.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ']],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_lutz_org.append({'type':'insertion',\n",
        "                 'text':['und','oder','so gilt','.','.','.','.','.'],\n",
        "                 'next':0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vOea6FflS4BL"
      },
      "outputs": [],
      "source": [
        "patterns_lutz_var = []\n",
        "\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_lutz_var.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'any': True,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'number': [['Sing']],\n",
        "                 'case':[['Nom']],         # the Noun should be nominative\n",
        "                 'next':1})                           # next node\n",
        "\n",
        "            \n",
        "#######################################################\n",
        "#1\n",
        "patterns_lutz_var.append({'type':'insertion',\n",
        "                 'dependent':['gender'],\n",
        "                 'dependency': -1,\n",
        "                 'position': 'before',                      # only possible with insertion\n",
        "                 'Fem':['eine','jede','keine','nicht jede'],  \n",
        "                 'Masc':['ein','jeder','kein','nicht jeder'],\n",
        "                 'Neut':['ein','jedes','kein','nicht jedes'],\n",
        "                 'next':2})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_lutz_var.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':[''],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 '':['ist'],        # use when the last node is Plural\n",
        "                 'next':3})                            \n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_lutz_var.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':64,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['ein','eine'],[]]],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_lutz_var.append({'type':'insertion',\n",
        "                 'text':['und','oder','so gilt','.','.','.','.','.'],\n",
        "                 'next':0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YZIyH50tM3NZ"
      },
      "outputs": [],
      "source": [
        "patterns_1 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_1.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'any': True,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'case':[['','Nom'],['Nom']],         # the Noun should be nominative\n",
        "                 'add_det': [1,0],                    # add an article in front of the sequence\n",
        "                 'next':1})                           # next node\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_1.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':['number'],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 'Plur':['sind','sind nicht'],        # use when the last node is Plural\n",
        "                 'Sing':['ist','ist nicht'],          # use when teh last node is Singular\n",
        "                 'next':2})                            \n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_1.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_1.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns_1b = []\n",
        "\n",
        "\n",
        "patterns_1b.append({'type':'insertion',\n",
        "                 'text':['der','die','das'],  \n",
        "                 'next':1})\n",
        "#######################################################\n",
        "#1\n",
        "patterns_1b.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'dependent':[['',['gender','number']],[['gender','number']]],\n",
        "                 'dependency': -1,                       \n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'case':[['','Nom'],['Nom']],         # the Noun should be nominative\n",
        "                 'next':2})                           # next node\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_1b.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':['number'],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 'Plur':['sind','sind nicht'],        # use when the last node is Plural\n",
        "                 'Sing':['ist','ist nicht'],          # use when teh last node is Singular\n",
        "                 'next':3})                            \n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_1b.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_1b.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9a6qQOkeM3Na"
      },
      "outputs": [],
      "source": [
        "patterns_2 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'text':['ich','du','er','sie','es','wir'], #'ihr' is not corrected labeled by Spacy\n",
        "                 'next':1})\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'dependent':['number','person'],\n",
        "                 'dependency': -1,\n",
        "                 'Plur 1':['dürfen', 'müssen', 'dürfen nicht', 'müssen nicht', 'können', 'können nicht'],\n",
        "                 'Plur 2':['dürft', 'müsst', 'dürft nicht', 'müsst nicht', 'könnt', 'könnt nicht'],\n",
        "                 'Plur 3':['dürfen', 'müssen', 'dürfen nicht', 'müssen nicht', 'können', 'können nicht'], \n",
        "                 'Sing 1':['darf','muss','darf nicht','muss nicht','kann','kann nicht'],\n",
        "                 'Sing 2':['darfst','musst','darfst nicht','musst nicht','kannst','kannst nicht'],\n",
        "                 'Sing 3':['darf','muss','darf nicht','muss nicht','kann','kann nicht'],  \n",
        "                 'next':2})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_2.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':20,\n",
        "                 'pos':[['VERB'],['AUX','VERB']],\n",
        "                 'verbform':[['Inf'],['','Inf']],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zWi4HleGM3Nc"
      },
      "outputs": [],
      "source": [
        "patterns_3 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': True,\n",
        "                 'num_samples':20,\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],\n",
        "                 'case':[['','Nom'],['Nom']],\n",
        "                 'add_det': [1,0],\n",
        "                 'rand_next': [1,4]})               # list of next nodes to choose from randomly\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_3.append({'type':'insertion',\n",
        "                 'dependent':['number'],\n",
        "                 'dependency': -1,\n",
        "                 'Plur':['sind','sind nicht'],\n",
        "                 'Sing':['ist','ist nicht'],\n",
        "                 'rand_next': [2,5]})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_3.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'dependent':[[['number','person']],['',['number','person']]],\n",
        "                 'dependency': -1,\n",
        "                 'any': False,\n",
        "                 'num_samples': 100,\n",
        "                 'pos':[['VERB'],['AUX','VERB']],\n",
        "                 #'num':[['Sing'],['','Sing']],\n",
        "                 #'person':[[3],['',3]],\n",
        "                 'verbform':[['Fin'],['','Fin']],\n",
        "                 'next':3,\n",
        "                 'if_failed':1})                       # where to continue if no option is found\n",
        "\n",
        "#######################################################\n",
        "#5\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':90,\n",
        "                 'pos':[['ADJ'],['AUX']],\n",
        "                 'next':3,\n",
        "                 'if_failed':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "patterns_3b = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_3b.append({'type':'insertion',\n",
        "                 'text':['der','die','das'],\n",
        "                 'next':1})\n",
        "#######################################################\n",
        "#1\n",
        "patterns_3b.append({'type':'generate',\n",
        "                 'dependent':[['',['gender','number']],[['gender','number']]],\n",
        "                 'dependency': -1,   \n",
        "                 'any': False,\n",
        "                 'num_samples':20,\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],\n",
        "                 'case':[['','Nom'],['Nom']],\n",
        "                 'rand_next': [2,5]})               # list of next nodes to choose from randomly\n",
        "#######################################################\n",
        "#2\n",
        "patterns_3b.append({'type':'insertion',\n",
        "                 'dependent':['number'],\n",
        "                 'dependency': -1,\n",
        "                 'Plur':['sind','sind nicht'],\n",
        "                 'Sing':['ist','ist nicht'],\n",
        "                 'rand_next': [3,6]})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_3b.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_3b.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n",
        "#######################################################\n",
        "#5\n",
        "patterns_3b.append({'type':'generate',\n",
        "                 'dependent':[[['number','person']],['',['number','person']]],\n",
        "                 'dependency': -1,\n",
        "                 'any': False,\n",
        "                 'num_samples': 100,\n",
        "                 'pos':[['VERB'],['AUX','VERB']],\n",
        "                 #'num':[['Sing'],['','Sing']],\n",
        "                 #'person':[[3],['',3]],\n",
        "                 'verbform':[['Fin'],['','Fin']],\n",
        "                 'next':4,\n",
        "                 'if_failed':2})                       # where to continue if no option is found\n",
        "\n",
        "#######################################################\n",
        "#6\n",
        "patterns_3b.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':90,\n",
        "                 'pos':[['ADJ']],\n",
        "                 'next':4,\n",
        "                 'if_failed':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_dep(pattern,found_lst):\n",
        "    translate_dict = {'number':'Number','person':'Person','gender':'Gender'}\n",
        "\n",
        "    dependency = pattern['dependency']\n",
        "    flatten = [item for sublist in pattern['dependent']for item in sublist if item]\n",
        "    dependencies = list(set([item for sublist in flatten for item in sublist]))\n",
        "    for dep in dependencies:\n",
        "        pattern[dep] = []\n",
        "\n",
        "    for possibility in pattern['dependent']:\n",
        "\n",
        "        for dep in dependencies:\n",
        "            pattern[dep].append([])\n",
        "        for dep_lst in possibility:\n",
        "            for key in dependencies:\n",
        "\n",
        "                if key in dep_lst:                \n",
        "                    try:\n",
        "                        pattern[key][-1].append(found_lst[dependency]['morph'][-1][translate_dict[key]])\n",
        "                    except:\n",
        "                        if key == 'person':\n",
        "                            pattern[key][-1].append(3)  # a noun has no person tag, but it is 3rd person\n",
        "                        else:\n",
        "                            print(key)\n",
        "                            raise(Exception)\n",
        "                else: \n",
        "                    pattern[key][-1].append('')\n",
        "\n",
        "\n",
        "\n",
        "    return(pattern)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "yw5uHQn_M3Ne"
      },
      "outputs": [],
      "source": [
        "def check_gender(genders, idx,doc):\n",
        "    if len(genders) <= idx:\n",
        "        comp_genders = genders[0]\n",
        "    else:\n",
        "        comp_genders = genders[idx]\n",
        "\n",
        "    if type(comp_genders) != list:\n",
        "        comp_genders = [comp_genders]\n",
        "    checked = True\n",
        "    for i, gender in enumerate(comp_genders[:len(doc)]):\n",
        "\n",
        "        if gender == '':\n",
        "            pass\n",
        "        elif doc[i].morph.to_dict().get('Gender') != gender:\n",
        "            checked = False\n",
        "\n",
        "    return checked\n",
        "\n",
        "def check_pos(pos_tags, idx,doc):\n",
        "    pos = [item.pos_ for item in doc]\n",
        "\n",
        "    return pos == pos_tags[idx][:len(pos)]\n",
        "\n",
        "def check_word(words, idx,doc):\n",
        "    comp_words = words[idx]\n",
        "    checked = True\n",
        "    for i, word in enumerate(comp_words[:len(doc)]):\n",
        "        if word == []:\n",
        "            pass\n",
        "        elif word == doc[i].text:\n",
        "            checked = False\n",
        "            \n",
        "    return checked\n",
        "\n",
        "def check_case(cases, idx,doc):\n",
        "    if len(cases) <= idx:\n",
        "        comp_cases = cases[0]\n",
        "    else:\n",
        "        comp_cases = cases[idx]\n",
        "    checked = True\n",
        "    for i, case in enumerate(comp_cases[:len(doc)]):\n",
        "\n",
        "        if case == '':\n",
        "            pass\n",
        "        elif doc[i].morph.to_dict().get('Case') != case:\n",
        "            checked = False\n",
        "\n",
        "    return checked\n",
        "\n",
        "def check_num(nums, idx,doc):\n",
        "    if len(nums) <= idx:\n",
        "        comp_nums = nums[0]\n",
        "    else:\n",
        "        comp_nums = nums[idx]\n",
        "    checked = True\n",
        "    for i, num in enumerate(comp_nums[:len(doc)]):\n",
        "\n",
        "        if num == '':\n",
        "            pass\n",
        "        elif doc[i].morph.to_dict().get('Number') != num:\n",
        "            checked = False\n",
        "       \n",
        "    return checked\n",
        "\n",
        "\n",
        "def check_person(persons, idx,doc):\n",
        "    if len(persons) <= idx:\n",
        "        comp_persons = persons[0]\n",
        "    else:\n",
        "        comp_persons = persons[idx]\n",
        "    checked = True\n",
        "    for i, person in enumerate(comp_persons[:len(doc)]):\n",
        "\n",
        "        if person == '':\n",
        "            pass\n",
        "        elif str(doc[i].morph.to_dict().get('Person')) != str(person):\n",
        "            checked = False      \n",
        "    return checked\n",
        "\n",
        "def check_verbform(verbforms, idx,doc):\n",
        "    forms = verbforms[idx]\n",
        "    checked = True\n",
        "    for i, form in enumerate(forms[:len(doc)]):\n",
        "        if form == '':\n",
        "            pass\n",
        "        elif 'VerbForm' in doc[i].morph.to_dict().keys():\n",
        "            if doc[i].morph.to_dict().get('VerbForm') != form:\n",
        "            \n",
        "                checked = False\n",
        "        \n",
        "    return checked\n",
        "\n",
        "def check_all(conditions,max_idx,doc):\n",
        "    \n",
        "    for idx in range(max_idx):\n",
        "        checked = True\n",
        "        for condition in conditions:\n",
        "            checked = checked and condition(idx,doc)\n",
        "        if checked: \n",
        "            return True, idx\n",
        "        \n",
        "    return False, -1\n",
        "        \n",
        "    \n",
        "def store_words(doc):\n",
        "    text = ' '.join([item.text for item in doc])\n",
        "    dct = {'text':' '.join([item.text for item in doc]),\n",
        "           'pos':[],'morph':[],'dep':[]}\n",
        "    \n",
        "    for word in doc:\n",
        "        dct['pos'].append(word.pos_)\n",
        "        dct['dep'].append(word.dep_)\n",
        "        dct['morph'].append(word.morph.to_dict())\n",
        "        \n",
        "    return dct\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dWszgcEFM3Nf"
      },
      "outputs": [],
      "source": [
        "def get_pos_idx(pos_tags,n_gram):\n",
        "\n",
        "    idx_lst = []\n",
        "    for i, pos in enumerate(pos_tags):\n",
        "\n",
        "        if n_gram['pos'] == pos:\n",
        "            idx_lst.append(i)\n",
        "            \n",
        "    return idx_lst\n",
        "\n",
        "            \n",
        "def get_criteria_idx(criteria, n_gram):\n",
        "    poss_idx = np.asarray(criteria[0](n_gram))\n",
        "    if len(criteria) > 1: \n",
        "        for criterium in criteria[1:]: \n",
        "            poss_idx = np.settdif1d(poss_idx,criterium(n_gram))\n",
        "    try:\n",
        "        return poss_idx[0]\n",
        "    except:\n",
        "        return 0\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wWHxoZxkM3Nh"
      },
      "outputs": [],
      "source": [
        "def get_cond_len(pattern):\n",
        "    condition_length = []\n",
        "    for condition in pattern.values():\n",
        "        if type(condition) == list:\n",
        "            condition_length.append([len(cond) for cond in condition if type(cond) == list])\n",
        "    return condition_length\n",
        "\n",
        "def get_any(generated,prompt,pattern,check,lengths,min_return = 5):\n",
        "    found_words = []\n",
        "    condition_length = get_cond_len(pattern)\n",
        "    for sent in generated:\n",
        "            doc = nlp(prompt.strip() + ' ' + sent.strip())[-len(sent.strip().split()):]\n",
        "            for length in lengths:            \n",
        "                for j in range(len(doc)):   \n",
        "                    n_gram = doc[j:j+length]\n",
        "                    check_val, pattern_idx = check(n_gram)\n",
        "                    if check_val:\n",
        "                        if max([cond[pattern_idx] for cond in condition_length if len(cond) > pattern_idx]) == len(n_gram):\n",
        "                            found_words.append(n_gram)\n",
        "                        \n",
        "                        if len(found_words) > min_return: \n",
        "                            return found_words\n",
        "                        \n",
        "    return found_words\n",
        "    \n",
        "def get_first(generated,prompt,pattern,check,lengths, min_return = 5):\n",
        "    found_words = []\n",
        "    condition_length = get_cond_len(pattern)\n",
        "    for sent in generated:\n",
        "            doc = nlp(prompt.strip() + ' ' + sent.strip())[-len(sent.strip().split()):]\n",
        "            for length in lengths:            \n",
        "                n_gram = doc[:length]\n",
        "                check_val, pattern_idx = check(n_gram)\n",
        "                if check_val:\n",
        "                    if max([cond[pattern_idx] for cond in condition_length if len(cond) > pattern_idx]) == len(n_gram):\n",
        "                        found_words.append(n_gram)\n",
        "                        \n",
        "                    if len(found_words) > min_return: \n",
        "                        return found_words\n",
        "                    \n",
        "    return found_words\n",
        "\n",
        "\n",
        "def process_generative_pattern(pattern, prompt,found_lst):\n",
        "    anywhere = False\n",
        "    if 'any' in pattern.keys():\n",
        "        if pattern['any'] == True:\n",
        "            anywhere = True\n",
        "            \n",
        "    if 'num_samples' in pattern.keys():\n",
        "        num_samples = pattern['num_samples']\n",
        "    else:\n",
        "        num_samples = 5\n",
        "        \n",
        "    num_gpt_samples = 30\n",
        "    \n",
        "    if num_samples > num_gpt_samples:\n",
        "        num_gpt_samples = num_samples\n",
        "\n",
        "    conditions = []\n",
        "    criteria = []\n",
        "    lengths = []\n",
        "\n",
        "    ''' if 'dependent' in pattern.keys():\n",
        "        dependency = pattern['dependency']\n",
        "        if 'number' in pattern['dependent']:                \n",
        "            conditions.append(functools.partial(check_num,[[found_lst[dependency]['morph'][-1]['Number']]]))\n",
        "\n",
        "        if 'person' in pattern['dependent']:                \n",
        "            conditions.append(functools.partial(check_num,[[found_lst[dependency]['morph'][-1]['Person']]]))\n",
        "\n",
        "        if 'gender' in pattern['dependent']:                \n",
        "            conditions.append(functools.partial(check_gender,[[found_lst[dependency]['morph'][-1]['Gender']]]))\n",
        "    '''\n",
        "\n",
        "    if 'pos' in pattern.keys():\n",
        "        max_idx = len(pattern['pos'])\n",
        "        conditions.append(functools.partial(check_pos,pattern['pos']))\n",
        "        criteria.append(functools.partial(get_pos_idx,pattern['pos']))\n",
        "        lengths += [len(item) for item in pattern['pos']]\n",
        "        \n",
        "    if 'words' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_word,pattern['words']))\n",
        "\n",
        "    if 'number' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_num,pattern['number']))\n",
        "\n",
        "    if 'person' in pattern.keys():                \n",
        "        conditions.append(functools.partial(check_person,pattern['person']))\n",
        "\n",
        "    if 'gender' in pattern.keys():                \n",
        "        conditions.append(functools.partial(check_gender,pattern['gender']))\n",
        "\n",
        "    if 'case' in pattern.keys():                \n",
        "        conditions.append(functools.partial(check_case,pattern['case']))\n",
        "        \n",
        "    if 'verbform' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_verbform,pattern['verbform']))\n",
        "        \n",
        "   \n",
        "    lengths = list(set(lengths))\n",
        "    \n",
        "    check = functools.partial(check_all,conditions,max_idx)\n",
        "    get_idx = functools.partial(get_criteria_idx,criteria)\n",
        "        \n",
        "    if anywhere: \n",
        "        generated = gpt2_generate(prompt,num_return_sequences=num_gpt_samples)\n",
        "        found_words = get_any(generated,prompt,pattern,check,lengths, min_return = num_samples)\n",
        "        \n",
        "        if not found_words:\n",
        "            return '', 0\n",
        "        word = random.choice(found_words)    \n",
        "        \n",
        "    else: \n",
        "\n",
        "        found_words = gpt_sample_systematic(prompt,check,pattern)\n",
        "        if not found_words:\n",
        "            return '', 0\n",
        "        word = found_words[0]\n",
        "        \n",
        "        \n",
        "    word = store_words(word)\n",
        "\n",
        "    return word, get_idx(word)\n",
        "    \n",
        "        \n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "qytTxj3mM3Ni"
      },
      "outputs": [],
      "source": [
        "article_dict = {'Fem':'die','Masc':'der','Neut':'das'}\n",
        "\n",
        "def generate_patterns(prompt,patterns,loops,stop_pattern,print_loops=False):\n",
        "    next_pattern = 0\n",
        "    found_lst = []\n",
        "    count = 0\n",
        "    while count < loops:\n",
        "        \n",
        "        pattern = patterns[next_pattern]\n",
        "        before = False\n",
        "        if 'position' in pattern.keys():\n",
        "          if pattern['position'] == 'before':\n",
        "            before = True\n",
        "\n",
        "            \n",
        "        if pattern['type'] == 'generate':\n",
        "            if 'dependent' in pattern.keys():\n",
        "                pattern = parse_dep(pattern,found_lst)\n",
        "\n",
        "            found_word, index = process_generative_pattern(pattern,prompt,found_lst)\n",
        "            \n",
        "            if not found_word:\n",
        "                success = False\n",
        "                if not 'if_failed' in pattern.keys():\n",
        "                    print(prompt)\n",
        "                    \n",
        "                    print('''no completion found, please try again by pressing the play button\n",
        "                    of this cell''')\n",
        "\n",
        "                    return prompt\n",
        "            else:\n",
        "                success = True\n",
        "\n",
        "            if 'add_det' in pattern.keys() and success:\n",
        "                dependency = pattern['add_det'][index]\n",
        "                if pattern['add_det'][dependency] > - 1:\n",
        "                    \n",
        "                    gender = found_word['morph'][dependency]['Gender']\n",
        "                    number = found_word['morph'][dependency]['Number']\n",
        "                 \n",
        "                    if number == 'Plur':\n",
        "                        article = 'die'\n",
        "\n",
        "                    else: \n",
        "                        article = article_dict[gender]\n",
        "\n",
        "                    found_word['text'] = article + ' ' + found_word['text']\n",
        "        else:\n",
        "            success = True\n",
        "            if 'dependent' in pattern.keys():\n",
        "                query = ''\n",
        "                dependency = pattern['dependency']\n",
        "                if 'number' in pattern['dependent']:                \n",
        "                    query += found_lst[dependency]['morph'][-1]['Number']\n",
        "                if 'person' in pattern['dependent']:\n",
        "                    query += ' ' + str(found_lst[dependency]['morph'][-1]['Person'])\n",
        "\n",
        "                if 'gender' in pattern['dependent']:                \n",
        "                    query += found_lst[dependency]['morph'][-1]['Gender']\n",
        "\n",
        "\n",
        "                word = random.choice(pattern[query])\n",
        "\n",
        "            else: \n",
        "                word = random.choice(pattern['text'])\n",
        "\n",
        "            word_size = len(nlp(word))\n",
        "            doc = nlp(prompt + ' ' + word)[-word_size:]\n",
        "            found_word = store_words(doc)\n",
        "\n",
        "\n",
        "        if success: \n",
        "            if found_lst:                                 # not the first loop\n",
        "                if before:\n",
        "                  prompt_lst = prompt.split()\n",
        "                  prompt = ' '.join(prompt_lst[:-1] + [found_word['text'].strip()] + [prompt_lst[-1]])\n",
        "                  found_lst = [found_word] + found_lst\n",
        "                else: \n",
        "                  prompt += ' ' + found_word['text'].strip()\n",
        "                  found_lst.append(found_word)\n",
        "            else:\n",
        "                prompt = found_word['text'].strip()\n",
        "                found_lst.append(found_word)       \n",
        "\n",
        "            if 'rand_next' in pattern.keys():\n",
        "                next_pattern = random.choice(pattern['rand_next'])\n",
        "            else: \n",
        "                next_pattern = pattern['next']\n",
        "        else: \n",
        "            next_pattern = pattern['if_failed']\n",
        "            \n",
        "        if next_pattern == stop_pattern: \n",
        "            count +=1\n",
        "        if print_loops:\n",
        "          print(prompt)\n",
        "    return prompt\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TRL0dts-M3Nm"
      },
      "outputs": [],
      "source": [
        "prompt = '<|endoftext|>.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvCKtsj2b4jH"
      },
      "source": [
        "## Generating the pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "67OzVT_xb34J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "die Gründe\n",
            "die Gründe belegen\n",
            "die Gründe belegen denn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "die Gründe belegen denn die Erfahrung\n",
            "die Gründe belegen denn die Erfahrung ruht\n",
            "die Gründe belegen denn die Erfahrung ruht denn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "die Gründe belegen denn die Erfahrung ruht denn das freyes\n",
            "die Gründe belegen denn die Erfahrung ruht denn das freyes standhafte\n",
            "die Gründe belegen denn die Erfahrung ruht denn das freyes standhafte denn\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "die Gründe belegen denn die Erfahrung ruht denn das freyes standhafte denn die Art\n",
            "die Gründe belegen denn die Erfahrung ruht denn das freyes standhafte denn die Art neigt\n",
            "die Gründe belegen denn die Erfahrung ruht denn das freyes standhafte denn die Art neigt\n"
          ]
        }
      ],
      "source": [
        "number_patterns = 4\n",
        "print(generate_patterns(prompt, patterns_3,number_patterns,stop_pattern = 3,print_loops=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9SC2EFPaKAb"
      },
      "source": [
        "## The original scheme of Theo Lutz. \n",
        "Dependent on the corpus the model is trained on, the sequence of \"ist\" followed by an adjective is not too common. So you might need to run this cell a few times. Unfortunately the Kant model will almost always return \"absolute\" as adjective, if the model is required to follow the \"ist\". That is why the option \"any\" is set to True. Therefore the results are a bit random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "iAuhIsI5M3Nn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "jede\n",
            "jede Merkwürdigkeit\n",
            "jede Merkwürdigkeit ist\n",
            "jede Merkwürdigkeit ist besondere\n",
            "jede Merkwürdigkeit ist besondere .\n",
            "jede Merkwürdigkeit ist besondere . jede\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener .\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter .\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt jede\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt jede Denunction\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt jede Denunction ist\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt jede Denunction ist gewissen\n",
            "jede Merkwürdigkeit ist besondere . jede Zueignung ist eigener . nicht jede Launigkeit ist begründeter . nicht jede Ausserordentliche ist eingeräumter so gilt jede Denunction ist gewissen\n"
          ]
        }
      ],
      "source": [
        "number_patterns = 5\n",
        "print(generate_patterns(prompt, patterns_lutz_org,number_patterns,stop_pattern = 4,print_loops=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD4AaSPdNiLR"
      },
      "source": [
        "## Create multiple lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBwmFD8GM3No"
      },
      "outputs": [],
      "source": [
        "for k in range(10):\n",
        "    loop = random.choice([2,3])\n",
        "    loop = 2\n",
        "    print(generate_patterns(prompt,patterns_2,loop,stop_pattern = 3))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "llm_theo_lutz.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.6.7 ('tf36')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "e5ae3400ef1f554e4e93f8548b6dbb7210a84f0a63b5bda3df837b46d507e77c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
