{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andjoer/llm_poetry_generation/blob/main/colabs/llm_theo_lutz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrZ2myE2M3NO"
      },
      "source": [
        "# Inspired by Theo Lutz\n",
        "\n",
        "## Combining large language models and patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "click the 'Run all' button\n",
        "\n",
        "<img src = 'https://github.com/andjoer/llm_poetry_generation/blob/main/graphics/colab_en.jpg?raw=true'>\n",
        "\n",
        "German\n",
        "\n",
        "\n",
        "\n",
        "<img src = 'https://github.com/andjoer/llm_poetry_generation/blob/main/graphics/colab.jpg?raw=true'>"
      ],
      "metadata": {
        "id": "91QWM1NGy9OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "9wXHQY89M4Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkFCIitvM3NQ"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer,GPT2LMHeadModel, pipeline\n",
        "import numpy as np\n",
        "import torch\n",
        "import spacy\n",
        "import functools\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download \"de_core_news_lg\"\n",
        "\n",
        "nlp = spacy.load(\"de_core_news_lg\")"
      ],
      "metadata": {
        "id": "ElLwUBMRNOAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3rENvHMM3NT"
      },
      "source": [
        "## Defining the model to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsVgvkYIM3NU"
      },
      "outputs": [],
      "source": [
        "gpt2_model = \"Anjoe/kant-gpt2-large\"\n",
        "\n",
        "generator = pipeline('text-generation', model=gpt2_model,\n",
        "                 tokenizer=gpt2_model, framework = 'pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GEKYuevM3NW"
      },
      "outputs": [],
      "source": [
        "def gpt2_generate(input_text,max_length= 5, num_return_sequences=30):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "    max_length += tokenizer.encode(input_text,return_tensors='pt').size(1)\n",
        "    generated = generator(input_text, max_length=max_length,return_full_text = False, num_return_sequences=num_return_sequences)\n",
        "    \n",
        "    return [item['generated_text'] for item in generated]\n",
        "\n",
        "\n",
        "def gpt2_top_k(input_text,max_length = 10,num_return_sequences=30):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_model)\n",
        "    model = GPT2LMHeadModel.from_pretrained(gpt2_model,pad_token_id = tokenizer.eos_token_id)\n",
        "    input_ids = tokenizer.encode(input_text,return_tensors='pt')\n",
        "    max_length += input_ids.size(1)\n",
        "    start = input_ids.size()[1]\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample = True,\n",
        "        max_length = max_length,\n",
        "        top_k = num_return_sequences,\n",
        "        num_return_sequences = num_return_sequences,\n",
        "        early_stopping = True,\n",
        "        num_repeat_ngram_size = 2\n",
        "    )\n",
        "\n",
        "    return [tokenizer.decode(sample_output[start:],skip_special_tokens=True) for sample_output in output]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4JZco4bM3NX"
      },
      "source": [
        "## Define the patterns\n",
        "\n",
        "A sequence of \"nodes\" is defined."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patterns_lutz_org = []\n",
        "\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_lutz_org.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'any': True,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'number': [['Sing']],\n",
        "                 'case':[['Nom']],         # the Noun should be nominative\n",
        "                 'next':1})                           # next node\n",
        "\n",
        "            \n",
        "#######################################################\n",
        "#1\n",
        "patterns_lutz_org.append({'type':'insertion',\n",
        "                 'dependent':['gender'],\n",
        "                 'dependency': -1,\n",
        "                 'position': 'before',                      # only possible with insertion\n",
        "                 'Fem':['eine','jede','keine','nicht jede'],  \n",
        "                 'Masc':['ein','jeder','kein','nicht jeder'],\n",
        "                 'Neut':['ein','jedes','kein','nicht jedes'],\n",
        "                 'next':2})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_lutz_org.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':[''],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 '':['ist'],        # use when the last node is Plural\n",
        "                 'next':3})                            \n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_lutz_org.append({'type':'generate',\n",
        "                 'any': True,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ']],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_lutz_org.append({'type':'insertion',\n",
        "                 'text':['und','oder','so gilt','.','.','.','.','.'],\n",
        "                 'next':0})\n",
        "\n"
      ],
      "metadata": {
        "id": "r_1VKGOh-8jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patterns_lutz_var = []\n",
        "\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_lutz_var.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'any': True,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'number': [['Sing']],\n",
        "                 'case':[['Nom']],         # the Noun should be nominative\n",
        "                 'next':1})                           # next node\n",
        "\n",
        "            \n",
        "#######################################################\n",
        "#1\n",
        "patterns_lutz_var.append({'type':'insertion',\n",
        "                 'dependent':['gender'],\n",
        "                 'dependency': -1,\n",
        "                 'position': 'before',                      # only possible with insertion\n",
        "                 'Fem':['eine','jede','keine','nicht jede'],  \n",
        "                 'Masc':['ein','jeder','kein','nicht jeder'],\n",
        "                 'Neut':['ein','jedes','kein','nicht jedes'],\n",
        "                 'next':2})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_lutz_var.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':[''],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 '':['ist'],        # use when the last node is Plural\n",
        "                 'next':3})                            \n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_lutz_var.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':64,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['ein','eine'],[]]],\n",
        "                 'next':4})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_lutz_var.append({'type':'insertion',\n",
        "                 'text':['und','oder','so gilt','.','.','.','.','.'],\n",
        "                 'next':0})\n",
        "\n"
      ],
      "metadata": {
        "id": "vOea6FflS4BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZIyH50tM3NZ"
      },
      "outputs": [],
      "source": [
        "patterns_1 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_1.append({'type':'generate',                 # use the language model to produce text\n",
        "                 'any': True,                         # word can be found anywhere in the text\n",
        "                 'num_samples':20,                    # number of samples that comply with the criteria\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],     # should be either sequence adjective, noun or only noun\n",
        "                 'case':[['','Nom'],['Nom']],         # the Noun should be nominative\n",
        "                 'add_det': [1,0],                    # add an article in front of the sequence\n",
        "                 'next':1})                           # next node\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_1.append({'type':'insertion',                # insert a predefined sequence \n",
        "                 'dependent':['number'],              # property of the generated text it depends on\n",
        "                 'dependency': -1,                    # on which previous node it depends\n",
        "                 'Plur':['sind','sind nicht'],        # use when the last node is Plural\n",
        "                 'Sing':['ist','ist nicht'],          # use when teh last node is Singular\n",
        "                 'next':2})                            \n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_1.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_1.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a6qQOkeM3Na"
      },
      "outputs": [],
      "source": [
        "patterns_2 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'text':['ich','du','er','sie','es','wir'], #'ihr' is not corrected labeled by Spacy\n",
        "                 'next':1})\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'dependent':['number','person'],\n",
        "                 'dependency': -1,\n",
        "                 'Plur 1':['dürfen', 'müssen', 'dürfen nicht', 'müssen nicht', 'können', 'können nicht'],\n",
        "                 'Plur 2':['dürft', 'müsst', 'dürft nicht', 'müsst nicht', 'könnt', 'könnt nicht'],\n",
        "                 'Plur 3':['dürfen', 'müssen', 'dürfen nicht', 'müssen nicht', 'können', 'können nicht'], \n",
        "                 'Sing 1':['darf','muss','darf nicht','muss nicht','kann','kann nicht'],\n",
        "                 'Sing 2':['darfst','musst','darfst nicht','musst nicht','kannst','kannst nicht'],\n",
        "                 'Sing 3':['darf','muss','darf nicht','muss nicht','kann','kann nicht'],  \n",
        "                 'next':2})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_2.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':20,\n",
        "                 'pos':[['VERB'],['AUX','VERB']],\n",
        "                 'verbform':[['Inf'],['','Inf']],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_2.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWi4HleGM3Nc"
      },
      "outputs": [],
      "source": [
        "patterns_3 = []\n",
        "\n",
        "#######################################################\n",
        "#0\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': True,\n",
        "                 'num_samples':20,\n",
        "                 'pos':[['ADJ','NOUN'],['NOUN']],\n",
        "                 'case':[['','Nom'],['Nom']],\n",
        "                 'add_det': [1,0],\n",
        "                 'rand_next': [1,4]})               # list of next nodes to choose from randomly\n",
        "\n",
        "#######################################################\n",
        "#1\n",
        "patterns_3.append({'type':'insertion',\n",
        "                 'dependent':['number'],\n",
        "                 'dependency': -1,\n",
        "                 'Plur':['sind','sind nicht'],\n",
        "                 'Sing':['ist','ist nicht'],\n",
        "                 'rand_next': [2,5]})\n",
        "\n",
        "#######################################################\n",
        "#2\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':8,\n",
        "                 'pos':[['ADJ'],['DET','NOUN']],\n",
        "                 'case':[[''],['','Nom']],\n",
        "                 'words':[[],[['der','die','das'],[]]],\n",
        "                 'next':3})\n",
        "\n",
        "#######################################################\n",
        "#3\n",
        "patterns_3.append({'type':'insertion',\n",
        "                 'text':['denn'],\n",
        "                 'next':0})\n",
        "\n",
        "#######################################################\n",
        "#4\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples': 100,\n",
        "                 'pos':[['VERB'],['AUX','VERB']],\n",
        "                 'verbform':[['Inf'],['','Inf']],\n",
        "                 'next':3,\n",
        "                 'if_failed':1})                       # where to continue if no option is found\n",
        "\n",
        "#######################################################\n",
        "#5\n",
        "patterns_3.append({'type':'generate',\n",
        "                 'any': False,\n",
        "                 'num_samples':90,\n",
        "                 'pos':[['ADJ'],['AUX']],\n",
        "                 'next':3,\n",
        "                 'if_failed':2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw5uHQn_M3Ne"
      },
      "outputs": [],
      "source": [
        "def check_pos(pos_tags, idx,doc):\n",
        "    pos = [item.pos_ for item in doc]\n",
        "    return pos == pos_tags[idx]\n",
        "\n",
        "def check_word(words, idx,doc):\n",
        "    comp_words = words[idx]\n",
        "    checked = False\n",
        "    for i, word in enumerate(comp_words):\n",
        "        if word == []:\n",
        "            checked = True\n",
        "        elif word == doc[i].text:\n",
        "            checked = True\n",
        "            \n",
        "    return checked\n",
        "\n",
        "def check_case(cases, idx,doc):\n",
        "    comp_cases = cases[idx]\n",
        "    checked = False\n",
        "    for i, case in enumerate(comp_cases):\n",
        "        if case == '':\n",
        "            checked = True\n",
        "        elif doc[i].morph.to_dict()['Case'] == case:\n",
        "            checked = True\n",
        "            \n",
        "    return checked\n",
        "\n",
        "def check_num(nums, idx,doc):\n",
        "    comp_nums = nums[idx]\n",
        "    checked = False\n",
        "    for i, nums in enumerate(comp_nums):\n",
        "        if nums == '':\n",
        "            checked = True\n",
        "        elif doc[i].morph.to_dict()['Number'] == nums:\n",
        "            checked = True\n",
        "            \n",
        "    return checked\n",
        "\n",
        "def check_verbform(verbforms, idx,doc):\n",
        "    forms = verbforms[idx]\n",
        "    checked = False\n",
        "    for i, form in enumerate(forms):\n",
        "        if form == '':\n",
        "            checked = True\n",
        "        elif 'VerbForm' in doc[i].morph.to_dict().keys():\n",
        "            if doc[i].morph.to_dict()['VerbForm'] == form:\n",
        "            \n",
        "                checked = True\n",
        "            \n",
        "    return checked\n",
        "\n",
        "def check_all(conditions,max_idx,doc):\n",
        "    \n",
        "    for idx in range(max_idx):\n",
        "        checked = True\n",
        "        for condition in conditions:\n",
        "            checked = checked and condition(idx,doc)\n",
        "        if checked: \n",
        "            return True\n",
        "        \n",
        "    return False\n",
        "        \n",
        "    \n",
        "def store_words(doc):\n",
        "    text = ' '.join([item.text for item in doc])\n",
        "    dct = {'text':' '.join([item.text for item in doc]),\n",
        "           'pos':[],'morph':[],'dep':[]}\n",
        "    \n",
        "    for word in doc:\n",
        "        dct['pos'].append(word.pos_)\n",
        "        dct['dep'].append(word.dep_)\n",
        "        dct['morph'].append(word.morph.to_dict())\n",
        "        \n",
        "    return dct\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWszgcEFM3Nf"
      },
      "outputs": [],
      "source": [
        "def get_pos_idx(pos_tags,n_gram):\n",
        "    idx_lst = []\n",
        "    for i, pos in enumerate(pos_tags):\n",
        "\n",
        "        if n_gram['pos'] == pos:\n",
        "            idx_lst.append(i)\n",
        "            \n",
        "    return idx_lst\n",
        "\n",
        "            \n",
        "def get_criteria_idx(criteria, n_gram):\n",
        "    poss_idx = np.asarray(criteria[0](n_gram))\n",
        "    if len(criteria) > 1: \n",
        "        for criterium in criteria[1:]: \n",
        "            poss_idx = np.settdif1d(poss_idx,criterium(n_gram))\n",
        "        \n",
        "    return poss_idx[0]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWHxoZxkM3Nh"
      },
      "outputs": [],
      "source": [
        "def get_any(generated,check,lengths,min_return = 5):\n",
        "    found_words = []\n",
        "    for sent in generated:\n",
        "            doc = nlp(sent)\n",
        "            for length in lengths:            \n",
        "                for j in range(len(doc)-length):\n",
        "                    \n",
        "                    n_gram = doc[j:j+length]\n",
        "\n",
        "                    if check(n_gram):\n",
        "                        found_words.append(store_words(n_gram))\n",
        "                        \n",
        "                        if len(found_words) > min_return: \n",
        "                            return found_words\n",
        "                        \n",
        "    return found_words\n",
        "    \n",
        "def get_first(generated,check,lengths, min_return = 5):\n",
        "    found_words = []\n",
        "    for sent in generated:\n",
        "            doc = nlp(sent.strip())\n",
        "            for length in lengths:            \n",
        "                n_gram = doc[:length]\n",
        "                if check(n_gram):\n",
        "                    found_words.append(store_words(n_gram))\n",
        "                        \n",
        "                    if len(found_words) > min_return: \n",
        "                        return found_words\n",
        "                    \n",
        "    return found_words\n",
        "\n",
        "\n",
        "\n",
        "def process_generative_pattern(pattern, prompt):\n",
        "    anywhere = False\n",
        "    if 'any' in pattern.keys():\n",
        "        if pattern['any'] == True:\n",
        "            anywhere = True\n",
        "            \n",
        "    if 'num_samples' in pattern.keys():\n",
        "        num_samples = pattern['num_samples']\n",
        "    else:\n",
        "        num_samples = 5\n",
        "        \n",
        "    num_gpt_samples = 30\n",
        "    \n",
        "    if num_samples > num_gpt_samples:\n",
        "        num_gpt_samples = num_samples\n",
        "        \n",
        "    if anywhere: \n",
        "        generated = gpt2_generate(prompt,num_return_sequences=num_gpt_samples)\n",
        "    else: \n",
        "        generated = gpt2_generate(prompt,num_return_sequences=num_gpt_samples)\n",
        "\n",
        "    conditions = []\n",
        "    criteria = []\n",
        "    lengths = []\n",
        "    if 'pos' in pattern.keys():\n",
        "        max_idx = len(pattern['pos'])\n",
        "        conditions.append(functools.partial(check_pos,pattern['pos']))\n",
        "        criteria.append(functools.partial(get_pos_idx,pattern['pos']))\n",
        "        lengths += [len(item) for item in pattern['pos']]\n",
        "        \n",
        "    if 'words' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_word,pattern['words']))\n",
        "\n",
        "    if 'number' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_num,pattern['number']))\n",
        "        \n",
        "    if 'verbform' in pattern.keys():\n",
        "        conditions.append(functools.partial(check_verbform,pattern['verbform']))\n",
        "        \n",
        "   \n",
        "    lengths = list(set(lengths))\n",
        "    \n",
        "    check = functools.partial(check_all,conditions,max_idx)\n",
        "    get_idx = functools.partial(get_criteria_idx,criteria)\n",
        "    \n",
        "    \n",
        "        \n",
        "    if anywhere:\n",
        "        found_words = get_any(generated,check,lengths, min_return = num_samples)\n",
        "        \n",
        "    else:\n",
        "        found_words = get_first(generated,check,lengths, min_return = num_samples)\n",
        "        \n",
        "    if not found_words:\n",
        "        return '', 0\n",
        "    \n",
        "    word = random.choice(found_words)\n",
        "    return word, get_idx(word)\n",
        "    \n",
        "        \n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qytTxj3mM3Ni"
      },
      "outputs": [],
      "source": [
        "article_dict = {'Fem':'die','Masc':'der','Neut':'das'}\n",
        "\n",
        "def generate_patterns(prompt,patterns,loops,stop_pattern,print_loops=False):\n",
        "    next_pattern = 0\n",
        "    found_lst = []\n",
        "    count = 0\n",
        "    while count < loops:\n",
        "        \n",
        "        pattern = patterns[next_pattern]\n",
        "        before = False\n",
        "        if 'position' in pattern.keys():\n",
        "          if pattern['position'] == 'before':\n",
        "            before = True\n",
        "\n",
        "            \n",
        "        if pattern['type'] == 'generate':\n",
        "            found_word, index = process_generative_pattern(pattern,prompt)\n",
        "            \n",
        "            if not found_word:\n",
        "                success = False\n",
        "                if not 'if_failed' in pattern.keys():\n",
        "                    print(prompt)\n",
        "                    print('''no completion found, please try again by pressing the play button\n",
        "                    of this cell''')\n",
        "\n",
        "                    return prompt\n",
        "            else:\n",
        "                success = True\n",
        "\n",
        "            if 'add_det' in pattern.keys() and success:\n",
        "                dependency = pattern['add_det'][index]\n",
        "                if pattern['add_det'][dependency] > - 1:\n",
        "\n",
        "                    gender = found_word['morph'][dependency]['Gender']\n",
        "                    number = found_word['morph'][dependency]['Number']\n",
        "\n",
        "                    if number == 'Plur':\n",
        "                        article = 'die'\n",
        "\n",
        "                    else: \n",
        "                        article = article_dict[gender]\n",
        "\n",
        "                    found_word['text'] = article + ' ' + found_word['text']\n",
        "        else:\n",
        "            success = True\n",
        "            if 'dependent' in pattern.keys():\n",
        "                query = ''\n",
        "                dependency = pattern['dependency']\n",
        "                if 'number' in pattern['dependent']:                \n",
        "                    query += found_lst[dependency]['morph'][-1]['Number']\n",
        "                if 'person' in pattern['dependent']:\n",
        "                    query += ' ' + str(found_lst[dependency]['morph'][-1]['Person'])\n",
        "\n",
        "                if 'gender' in pattern['dependent']:                \n",
        "                    query += found_lst[dependency]['morph'][-1]['Gender']\n",
        "\n",
        "\n",
        "                word = random.choice(pattern[query])\n",
        "\n",
        "            else: \n",
        "                word = random.choice(pattern['text'])\n",
        "\n",
        "            word_size = len(nlp(word))\n",
        "            doc = nlp(prompt + ' ' + word)[-word_size:]\n",
        "            found_word = store_words(doc)\n",
        "\n",
        "\n",
        "        if success: \n",
        "            if found_lst:                                 # not the first loop\n",
        "                if before:\n",
        "                  prompt_lst = prompt.split()\n",
        "                  prompt = ' '.join(prompt_lst[:-1] + [found_word['text'].strip()] + [prompt_lst[-1]])\n",
        "                  found_lst = [found_word] + found_lst\n",
        "                else: \n",
        "                  prompt += ' ' + found_word['text'].strip()\n",
        "                  found_lst.append(found_word)\n",
        "            else:\n",
        "                prompt = found_word['text'].strip()\n",
        "                found_lst.append(found_word)       \n",
        "\n",
        "            if 'rand_next' in pattern.keys():\n",
        "                next_pattern = random.choice(pattern['rand_next'])\n",
        "            else: \n",
        "                next_pattern = pattern['next']\n",
        "        else: \n",
        "            next_pattern = pattern['if_failed']\n",
        "            \n",
        "        if next_pattern == stop_pattern: \n",
        "            count +=1\n",
        "        if print_loops:\n",
        "          print(prompt)\n",
        "    return prompt\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRL0dts-M3Nm"
      },
      "outputs": [],
      "source": [
        "prompt = '<|endoftext|>.'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the pattern"
      ],
      "metadata": {
        "id": "DvCKtsj2b4jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_patterns = 2\n",
        "print(generate_patterns(prompt, patterns_3,number_patterns,stop_pattern = 3,print_loops=True))"
      ],
      "metadata": {
        "id": "67OzVT_xb34J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The original scheme of Theo Lutz. \n",
        "Dependent on the corpus the model is trained on, the sequence of \"ist\" followed by an adjective is not too common. So you might need to run this cell a few times. Unfortunately the Kant model will almost always return \"absolute\" as adjective, if the model is required to follow the \"ist\". That is why the option \"any\" is set to True. Therefore the results are a bit random."
      ],
      "metadata": {
        "id": "e9SC2EFPaKAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAuhIsI5M3Nn"
      },
      "outputs": [],
      "source": [
        "number_patterns = 3\n",
        "print(generate_patterns(prompt, patterns_lutz_org,number_patterns,stop_pattern = 4,print_loops=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below the pattern is slightly modified, so that also \"ist ein/eine\" is allowed. The option that the next word may occur anywhere in the next sequence is set to False."
      ],
      "metadata": {
        "id": "KAno1Qklajfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_patterns = 2\n",
        "print(generate_patterns(prompt, patterns_lutz_var,number_patterns,stop_pattern = 4,print_loops=True))"
      ],
      "metadata": {
        "id": "HzV3F0asajGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create multiple lines"
      ],
      "metadata": {
        "id": "BD4AaSPdNiLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBwmFD8GM3No"
      },
      "outputs": [],
      "source": [
        "for k in range(10):\n",
        "    loop = random.choice([2,3])\n",
        "    loop = 2\n",
        "    print(generate_patterns(prompt,patterns_2,loop,stop_pattern = 3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "llm_theo_lutz.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}