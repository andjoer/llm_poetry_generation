{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Copied from Karoly Varasdi https://github.com/karoly-varasdi/de-wiktionary-parser \n",
    "# heavily modified to extract phonetic information for all words\n",
    "####################################################################################\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from reg_expressions import *\n",
    "\n",
    "def clean_up_string(string:str):\n",
    "    string = re.sub(wiki_link_string, r'\\2', string)\n",
    "    string = re.sub(specstring, ' ', string)\n",
    "    string = re.sub(quote_html, '\"', string)\n",
    "    string = re.sub(amp_html, '&', string)\n",
    "    for to_del_string in to_del_strings:\n",
    "        string = re.sub(to_del_string, '', string)\n",
    "    return string\n",
    "\n",
    "class wiktionary_dataframe:\n",
    "    \n",
    "    def __init__(self,file_path):\n",
    "        \n",
    "        self.word = []\n",
    "        self.word_sep = []\n",
    "        self.rhyme = []\n",
    "        self.ipa = []\n",
    "        self.label = []\n",
    "        self.generate_entries(file_path)\n",
    "        \n",
    "        self.df = pd.DataFrame(list(zip(self.word,self.ipa, self.label, self.rhyme,self.word_sep)), \n",
    "                  columns =['word','ipa', 'label', 'rhyme','sep'])\n",
    "        \n",
    "    def generate_entries(self, file_path):\n",
    "            '''Populates dictionary with noun information from the German wiktionary xml at file_path.'''\n",
    "            with open(file_path, 'r', encoding='utf-8') as wikif:\n",
    "                page_list = []\n",
    "                page_no = 0\n",
    "                print('Generating dictionary with word information from wiktionary source: {0}\\nThis may take several minutes . . .'.format(file_path))\n",
    "                for line in wikif:\n",
    "          \n",
    "                    page_list.append(line)\n",
    "                    if '</page>' in line:\n",
    "                        page_no += 1\n",
    "                        if page_no % 50000 == 0:\n",
    "                            print(page_no, 'pages processed')\n",
    "\n",
    "                        one_page_str = ''.join(page_list)\n",
    "                        # ## Cleaning up page before parsing:\n",
    "                        one_page_str = clean_up_string(one_page_str)\n",
    "\n",
    "                        page_list = one_page_str.splitlines()\n",
    "\n",
    "                        # German words\n",
    "                        if re.search(de_word_regex, one_page_str) is None:\n",
    "                            page_list = []\n",
    "                            continue\n",
    "\n",
    "                        word_match = re.search(de_headword_spaces_allowed_regex, one_page_str)\n",
    "                        if word_match:\n",
    "                            title_match = re.search(title_pattern, one_page_str)\n",
    "                            if title_match:\n",
    "                                adj_form = title_match.group('pagetitle')\n",
    "                                ## Ignore pages under wiktionary-namespaces:\n",
    "                                if re.match(namensraum_simple + colon, adj_form) or re.match(namensraum_simple + diskussion_colon, adj_form):\n",
    "                                    page_list = []\n",
    "                                    continue\n",
    "                            else:\n",
    "                                page_list = []\n",
    "                                continue\n",
    "                        else:\n",
    "                            page_list = []\n",
    "                            continue\n",
    "                            \n",
    "                        '''   for item in page_list:\n",
    "                            print(item)'''\n",
    "                        # call the next lower level parser function on a page with German noun info:\n",
    "                        self.parse_word_page(page_list)\n",
    "                        # delete entries with no usages (can happen if Abkürzung-only info in page)\n",
    "                       \n",
    "                        page_list = []\n",
    "                   \n",
    "                       \n",
    "                try:       \n",
    "                    print('Read {0} pages.'.format(page_no))\n",
    "                    print('Generated {0} entries.'.format(len(self)))\n",
    "                except: pass\n",
    "\n",
    "    def parse_word_page(self, page_list: list):\n",
    "            '''Parses a word page from the xml file, separates it into usages and calls the usage parser function on each usage.'''\n",
    "           \n",
    "            title = ''\n",
    "            rhyme_list = []\n",
    "            word_sep_list = []\n",
    "            ipa_list = []\n",
    "            label_itm = ''\n",
    "            \n",
    "            for index, line in enumerate(page_list):\n",
    "                try: \n",
    "                    title = re.search(r'(?<=<title>)(.*)(?=<\\/title>)', line,flags=0).group(0)\n",
    "                  \n",
    "\n",
    "                except: pass\n",
    "                \n",
    "                ipa_found = re.search(r'\\{\\{IPA\\}\\}', line)\n",
    "                if ipa_found: \n",
    "                    try: \n",
    "                        ipa_text = re.finditer(r'\\{\\{Lautschrift\\|.*?\\}\\}', line,flags=0)\n",
    "                        for item in ipa_text: \n",
    "                            ipa_list.append(re.search(r'(?<=\\{\\{Lautschrift\\|)(.*)(?=\\}\\})', item.group(0),flags=0).group(0))\n",
    "            \n",
    "                    except: pass\n",
    "\n",
    "                rhyme_found = re.search(r'\\{\\{Reime\\}\\}', line)\n",
    "                if rhyme_found: \n",
    "                    try: \n",
    "                        rhyme_text = re.finditer(r'\\{\\{Reim\\|.*?\\|Deutsch\\}\\}', line,flags=0)\n",
    "                        for item in rhyme_text: \n",
    "                            rhyme_list.append(re.search(r'(?<=\\{\\{Reim\\|)(.*)(?=\\|Deutsch\\}\\})', item.group(0),flags=0).group(0))\n",
    "            \n",
    "                    except: pass\n",
    "                    \n",
    "                sep_found = re.search(r'\\{\\{Worttrennung\\}\\}', line)\n",
    "                if sep_found:\n",
    "                    try: \n",
    "                        sep_text = page_list[index+1]\n",
    "                        sep_text = re.sub(r'\\{\\{[^)]*\\}\\}', '', sep_text)\n",
    "                        sep_text = re.sub(r'\\'\\'[^)]*\\'\\'', '', sep_text)\n",
    "                        sep_text = re.sub(r'[:,]', '', sep_text)\n",
    "                        sep_text = sep_text.split()\n",
    "                        word_sep_list.append(sep_text)\n",
    "                        \n",
    "                    except:pass  \n",
    "                        \n",
    "                rhyme_found = re.search(r'{\\{Wortart\\|', line)\n",
    "                if rhyme_found: \n",
    "                    try: \n",
    "                        rhyme_text = re.finditer(r'\\{\\{Wortart\\|.*?\\|Deutsch\\}\\}', line,flags=0)\n",
    "                        for item in rhyme_text: \n",
    "                            label_itm = (re.search(r'(?<=\\{\\{Wortart\\|)(.*)(?=\\|Deutsch\\}\\})', item.group(0),flags=0).group(0)).lower()\n",
    "                    except: pass\n",
    "                        \n",
    "                    \n",
    "                \n",
    "                end_found = re.search(r'==== {{Übersetzungen}} ====',line)\n",
    "                if end_found: \n",
    "                    break\n",
    "                    \n",
    "               \n",
    "            self.word.append(title.lower())  \n",
    "            self.ipa.append(ipa_list)\n",
    "            self.word_sep.append(word_sep_list)\n",
    "            self.rhyme.append(rhyme_list)\n",
    "            self.label.append(label_itm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dictionary with word information from wiktionary source: wiktionary_dump.xml\n",
      "This may take several minutes . . .\n",
      "50000 pages processed\n",
      "100000 pages processed\n",
      "150000 pages processed\n",
      "200000 pages processed\n",
      "250000 pages processed\n",
      "300000 pages processed\n",
      "350000 pages processed\n",
      "400000 pages processed\n",
      "450000 pages processed\n",
      "500000 pages processed\n",
      "550000 pages processed\n",
      "600000 pages processed\n",
      "650000 pages processed\n",
      "700000 pages processed\n",
      "750000 pages processed\n",
      "800000 pages processed\n",
      "850000 pages processed\n",
      "900000 pages processed\n",
      "950000 pages processed\n",
      "1000000 pages processed\n",
      "1050000 pages processed\n",
      "1100000 pages processed\n",
      "1150000 pages processed\n",
      "Read 1180818 pages.\n"
     ]
    }
   ],
   "source": [
    "fname = 'wiktionary_dump'\n",
    "\n",
    "wiktionary = wiktionary_dataframe(fname + '.xml')\n",
    "\n",
    "wiktionary.df.to_csv('wiktionary_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0        word                ipa       label      rhyme  \\\n",
      "0           0       hallo         ['haˈloː']  substantiv     ['oː']   \n",
      "1           1  subfamilia               ['']  substantiv         []   \n",
      "2           2   subregnum    ['zʊpˈʁeːɡnʊm']  substantiv         []   \n",
      "3           3  subdivisio  ['zʊpdiˈviːzi̯o']  substantiv         []   \n",
      "4           4      phylum        ['ˈfyːlʊm']  substantiv  ['yːlʊm']   \n",
      "\n",
      "                                         sep  \n",
      "0                    [['Hal·lo', 'Hal·los']]  \n",
      "1     [['Sub·fa·mi·lia', 'Sub·fa·mi·li·ae']]  \n",
      "2            [['Sub·reg·num', 'Sub·reg·na']]  \n",
      "3  [['Sub·di·vi·sio', 'Sub·di·vi·si·o·nes']]  \n",
      "4                    [['Phy·lum', 'Phy·la']]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('wiktionary_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
