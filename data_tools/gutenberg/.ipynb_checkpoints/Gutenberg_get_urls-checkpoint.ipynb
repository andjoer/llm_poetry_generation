{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2339 authors\n"
     ]
    }
   ],
   "source": [
    "author_url_lst = []\n",
    "\n",
    "from string import ascii_lowercase\n",
    "url_lst = []\n",
    "for letter in ascii_lowercase:\n",
    "    url_lst.append('https://www.projekt-gutenberg.org/autoren/info/autor-'+letter+'.html')\n",
    "    \n",
    "for URL in url_lst:\n",
    "\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.findAll('td',attrs={'class':'left'})\n",
    "    for result in results:\n",
    "        link = result.find('a')\n",
    "        try:\n",
    "            author_url_lst.append('https://www.projekt-gutenberg.org/autoren'+ link['href'][2:])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "print('found '+ str(len(author_url_lst)) + ' authors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 authors\n",
      "processed 100 authors\n",
      "processed 200 authors\n",
      "processed 300 authors\n",
      "processed 400 authors\n",
      "processed 500 authors\n",
      "processed 600 authors\n",
      "processed 700 authors\n",
      "processed 800 authors\n",
      "processed 900 authors\n",
      "processed 1000 authors\n",
      "processed 1100 authors\n",
      "processed 1200 authors\n",
      "processed 1300 authors\n",
      "processed 1400 authors\n",
      "processed 1500 authors\n",
      "processed 1600 authors\n",
      "processed 1700 authors\n",
      "processed 1800 authors\n",
      "processed 1900 authors\n",
      "processed 2000 authors\n",
      "processed 2100 authors\n",
      "processed 2200 authors\n",
      "processed 2300 authors\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Checking if the authors are not from a too ancient time\n",
    "'''\n",
    "\n",
    "poetry_urls = []\n",
    "\n",
    "author_url_lst_cleaned = []\n",
    "\n",
    "min_year = 1730\n",
    "\n",
    "for i, URL in enumerate(author_url_lst):\n",
    "\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        information = soup.findAll('p')\n",
    "        infotext_comp = ''\n",
    "        for info in information:\n",
    "            infotext_comp += info.getText()\n",
    "        \n",
    "        years = re.findall(r'\\d{4}', infotext_comp)\n",
    "        \n",
    "        years = [int(year) for year in years]\n",
    "        if years:\n",
    "            if min(years) > min_year:\n",
    "                author_url_lst_cleaned.append(URL)\n",
    "                \n",
    "        if i % 100 == 0:\n",
    "            print('processed ' + str(i) + ' authors')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 authors\n",
      "processed 100 authors\n",
      "processed 200 authors\n",
      "processed 300 authors\n",
      "processed 400 authors\n",
      "processed 500 authors\n",
      "processed 600 authors\n",
      "processed 700 authors\n",
      "processed 800 authors\n",
      "processed 900 authors\n",
      "processed 1000 authors\n",
      "processed 1100 authors\n",
      "processed 1200 authors\n",
      "processed 1300 authors\n",
      "processed 1400 authors\n",
      "processed 1500 authors\n",
      "processed 1600 authors\n",
      "processed 1700 authors\n",
      "processed 1800 authors\n",
      "processed 1900 authors\n"
     ]
    }
   ],
   "source": [
    "poetry_urls = []\n",
    "for i, URL in enumerate(author_url_lst_cleaned):\n",
    "    try:\n",
    "        page = requests.get(URL)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        archived = soup.find('div',attrs={'class':'archived'})\n",
    "        titles = archived.findAll('li')\n",
    "        for title in titles:\n",
    "            try:\n",
    "                anchortext = (title.find('a').contents[0]).lower()\n",
    "                link = 'https://www.projekt-gutenberg.org/' + title.find('a')['href'][6:]\n",
    "                poem = re.search(\"gedicht\", anchortext)\n",
    "                if poem:\n",
    "                    poetry_urls.append(link)\n",
    "            except:\n",
    "                pass\n",
    "        if i % 100 == 0:\n",
    "            print('processed ' + str(i) + ' authors')\n",
    "    except: \n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"poem_url_list\", \"rb\") as file:   \n",
    "    poetry_urls = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "print(len(poetry_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
