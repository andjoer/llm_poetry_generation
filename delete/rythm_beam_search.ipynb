{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andjoer/llm_poetry_generation/blob/main/colabs/LLM_alliterations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83YQtxlRLHyw"
   },
   "source": [
    "# Creating alliteration patterns with large language models and beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyP62vGZLI9w"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yvxh2q6YLHy1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SYp5zlU_LHy4"
   },
   "outputs": [],
   "source": [
    "model_name = \"Anjoe/german-poetry-gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jsCo6ZhOLHy5"
   },
   "outputs": [],
   "source": [
    "def create_word_start_mask(tokenizer):    \n",
    "    word_start_mask =  np.zeros(len(tokenizer))\n",
    "    for i in range(len(tokenizer)):\n",
    "        if tokenizer.decode(i)[0] == ' ':\n",
    "            word_start_mask[i] = 1\n",
    "    return word_start_mask\n",
    "        \n",
    "def perplexity(text):\n",
    "    device = model.device\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / end_loc).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bUaToIPFLHy7"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "# Modified so that it works more on a word level. \n",
    "# Example \"das Denkende\" and \"das Denken\" are the same n-gram. \n",
    "################################################################################\n",
    "\n",
    "def _get_word_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx]\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_word_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "\n",
    "    cur_len = len(prev_input_ids)\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len])\n",
    "\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_word_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, \n",
    "                                   num_hypos: int, cur_len: int, word_start_mask: np.array):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    prev_input_ids = [[item for item in prev_inputs.tolist() if word_start_mask[item]==1] \n",
    "                      for prev_inputs in prev_input_ids] # MODIFICATION\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_word_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_word_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "\n",
    "    return banned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sK4p6HwTLHy9"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "################################################################################\n",
    "\n",
    "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "    return banned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9mhzTzA3LHy_"
   },
   "outputs": [],
   "source": [
    "def find_word_beginning(token_ids):    \n",
    "    for j in range(1,len(token_ids)):\n",
    "                possible_beginning = tokenizer.decode(token_ids[-j])\n",
    "                if possible_beginning[0] == ' ' and possible_beginning.strip():\n",
    "                    return possible_beginning.strip(),j\n",
    "                    \n",
    "    else:\n",
    "        return False,0\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZvKnn-fULHy_"
   },
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "class alit_logits(LogitsProcessor):\n",
    "    def __init__(self, tokenizer,\n",
    "              first_stress = 0,  \n",
    "              word_beginnings = False,\n",
    "              ngram_size_words = 2,\n",
    "              ngram_size_tokens = 4,\n",
    "              max_word_len = 4,\n",
    "              len_rand = True,\n",
    "              nucleus_sampling = False,\n",
    "              top_p = 0.1,\n",
    "              len_metrum = 2,\n",
    "              len_verse = 10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_beginnings = word_beginnings\n",
    "        self.ngram_size_words = ngram_size_words\n",
    "        self.ngram_size_tokens = ngram_size_tokens\n",
    "        self.word_start_mask = create_word_start_mask(tokenizer)\n",
    "        self.max_word_len = max_word_len\n",
    "        self.len_rand = len_rand\n",
    "        self.nucleus_sampling = nucleus_sampling\n",
    "        self.top_p = top_p\n",
    "        self.first_stress = first_stress\n",
    "        self.len_metrum = len_metrum\n",
    "        self.delim = tokenizer.encode('.')+ tokenizer.encode(',')\n",
    "        self.new_line = tokenizer.encode('\\n')\n",
    "        self.len_verse = len_verse\n",
    "        self.first_stress = first_stress\n",
    "        \n",
    "        with open(r'notstressed', 'rb') as f:\n",
    "            self.lst_0 = pickle.load(f)\n",
    "    \n",
    "        with open(r'stressed', 'rb') as f:\n",
    "            self.lst_1 = pickle.load(f)\n",
    "    \n",
    "        with open(r'notstressed_start', 'rb') as f:\n",
    "            self.lst_0_s = pickle.load(f)\n",
    "    \n",
    "        with open(r'stressed_start', 'rb') as f:\n",
    "            self.lst_1_s = pickle.load(f)\n",
    "            \n",
    "        with open(r'stressed_start', 'rb') as f:\n",
    "            self.lst_1_s = pickle.load(f)\n",
    "        self.rythm_df = pd.read_csv('word_rythm.csv')\n",
    "        \n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        banned_tokens = []\n",
    "    \n",
    "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):    \n",
    "\n",
    "            banned = []\n",
    "            last_word, word_len = find_word_beginning(beam_input_ids)\n",
    "            text = self.tokenizer.decode(beam_input_ids[1:])  # start behind end of text token\n",
    "            \n",
    "            words = re.sub('[\\W_]+', ' ', text).split()[:-1] #stop before last word that is not final\n",
    "            num_syll = 0\n",
    "            if words:\n",
    "                for word in words:\n",
    "                    num_syll += self.rythm_df.loc[(self.rythm_df['word'] == word)]['num_syll'].values[0]\n",
    "\n",
    "            if num_syll % self.len_metrum == 0:\n",
    "                next_stress = (1-self.first_stress)**2\n",
    "            else:\n",
    "                next_stress = self.first_stress\n",
    "                \n",
    "            if next_stress == 1:\n",
    "                target_dict = self.lst_1\n",
    "                target_dict_s = self.lst_1_s\n",
    "            else:\n",
    "                target_dict = self.lst_0\n",
    "                target_dict_s = self.lst_0_s\n",
    "                \n",
    "            if num_syll > self.len_verse - 4:\n",
    "                keep = self.delim + self.new_line\n",
    "            else: \n",
    "                keep = self.delim\n",
    "            for i in range(len(tokenizer)):\n",
    "                \n",
    "                poss_word = str(list(beam_input_ids.cpu().detach().numpy())[-word_len:] + [i])\n",
    "                \n",
    "                probability = max(target_dict[poss_word],\n",
    "                        target_dict_s[i])\n",
    "                \n",
    "                if i in keep and num_syll > 0:\n",
    "                    probability = 1\n",
    "                    \n",
    "                if probability == 0:\n",
    "                    banned.append(i)\n",
    "                    \n",
    "            banned_tokens.append(banned)\n",
    "            \n",
    "\n",
    "        num_batch_hypotheses = scores.shape[0]\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_word_tokens = _calc_banned_word_ngram_tokens(self.ngram_size_words,\n",
    "                                                            input_ids,\n",
    "                                                            num_batch_hypotheses,\n",
    "                                                            cur_len,\n",
    "                                                            self.word_start_mask)\n",
    "        \n",
    "        banned_token_tokens = _calc_banned_ngram_tokens(self.ngram_size_tokens, \n",
    "                                                        input_ids, \n",
    "                                                        num_batch_hypotheses, \n",
    "                                                        cur_len)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (len(banned_tokens)): \n",
    "            banned_tokens[i] += banned_word_tokens[i] + banned_token_tokens[i]\n",
    "\n",
    "        for i, banned_token in enumerate(banned_tokens):\n",
    "            scores[i, banned_token] = -float(\"inf\")\n",
    "            \n",
    "        ##############################################################\n",
    "        # top p\n",
    "        if self.nucleus_sampling:\n",
    "     \n",
    "            m = torch.nn.Softmax(dim=1)\n",
    "            scores_sm = m(scores).cpu().detach().numpy()\n",
    "            for i in range(num_batch_hypotheses):\n",
    "\n",
    "                scores_sm_sorted = -np.sort(-scores_sm[i,:])\n",
    "\n",
    "                prob = 0\n",
    "                j = 0 \n",
    "\n",
    "                while prob < self.top_p:\n",
    "                    prob += scores_sm_sorted[i]\n",
    "                    j += 1\n",
    "\n",
    "                j = max(j,1)\n",
    "                candidate_idx = np.argsort(-scores_sm)[0][:j]\n",
    "\n",
    "                score_idx = random.choice(candidate_idx)\n",
    "\n",
    "                scores[i,:score_idx] = -float(\"inf\")\n",
    "                scores[i,score_idx+1:] = -float(\"inf\")\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P9t5-zi-LHzB"
   },
   "outputs": [],
   "source": [
    "def check_beginnings(letter,decoded,keep_tokens = ['.'],word_beginnings = True):\n",
    "    \n",
    "    if word_beginnings:\n",
    "        word_beginning = decoded[0] == ' '\n",
    "    else:\n",
    "        word_beginning = True\n",
    "        \n",
    "    decoded = decoded.strip()\n",
    "    if decoded != '':\n",
    "        start_letter = decoded[0].lower() != letter\n",
    "    else: \n",
    "        start_letter = True\n",
    "    not_alpha =  not decoded.isalpha()\n",
    "    \n",
    "    not_keep = decoded not in keep_tokens\n",
    "    if (word_beginning and start_letter or not_alpha) and not_keep :\n",
    "        return True\n",
    "        \n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l6KjujAfLHzD"
   },
   "outputs": [],
   "source": [
    "def create_block_token_dict(tokenizer,letters,keep_tokens,word_beginnings):\n",
    "    block_token_dict = {}\n",
    "\n",
    "    for i, letter in enumerate(letters):\n",
    "        block_tokens = []\n",
    "        for j in range(len(tokenizer)):\n",
    "            decoded = tokenizer.decode(j)\n",
    "            if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = word_beginnings):\n",
    "                block_tokens.append(j)\n",
    "        block_token_dict[letters[i-1]] = [block_tokens]\n",
    "\n",
    "    if word_beginnings:\n",
    "        for i, letter in enumerate(letters):\n",
    "            block_tokens = []\n",
    "            for j in range(len(tokenizer)):\n",
    "                decoded = tokenizer.decode(j)\n",
    "                if check_beginnings(letter,decoded,keep_tokens = keep_tokens,word_beginnings = False):\n",
    "                    block_tokens.append(j)\n",
    "            block_token_dict[letters[i-1]].append(block_tokens)\n",
    "            \n",
    "    return block_token_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p1DKBu6DLHzE"
   },
   "outputs": [],
   "source": [
    "def create_aliterations(prompt, letters, tokenizer,\n",
    "                       word_beginnings = False,\n",
    "                       max_length = 24,\n",
    "                       num_beams = 15,\n",
    "                       num_return_beams = 14,\n",
    "                       ngram_size_words = 2,\n",
    "                       ngram_size_tokens = 4,\n",
    "                       max_word_len = 4,\n",
    "                       len_rand = False,\n",
    "                       nucleus_sampling = False,\n",
    "                       top_p = 0.1 \n",
    "                       ):\n",
    "\n",
    "    if num_beams < num_return_beams:\n",
    "        print('warning: setting number of return beams equal to number of beams')\n",
    "        num_return_beams = num_beams\n",
    "    prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "    prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "    beam_scorer = BeamSearchScorer(\n",
    "        batch_size = prompt_tokenized.shape[0],\n",
    "        num_beams = num_beams,\n",
    "        num_beam_hyps_to_keep = num_return_beams,\n",
    "        device=model.device\n",
    "    )\n",
    "\n",
    "    \n",
    "    logits_processor = LogitsProcessorList([alit_logits(tokenizer,\n",
    "                                                        word_beginnings = word_beginnings,\n",
    "                                                        ngram_size_words = ngram_size_words,\n",
    "                                                        ngram_size_tokens = ngram_size_tokens,\n",
    "                                                        max_word_len = max_word_len,\n",
    "                                                        len_rand = len_rand,\n",
    "                                                        nucleus_sampling = nucleus_sampling,\n",
    "                                                        top_p = top_p)])\n",
    "\n",
    "    generated = model.beam_search(\n",
    "        torch.cat([prompt_tokenized] * num_beams),\n",
    "        beam_scorer,\n",
    "        logits_processor = logits_processor,\n",
    "        stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    "    )\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1TKh3s-HLHzG",
    "outputId": "2d02f2c3-0329-4c1b-bba9-5e6b25cbe715"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'notstressed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f465da2c3822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mlen_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# make the maximum number of tokens for a word random\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                 \u001b[0mnucleus_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# do additional top p sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                 top_p = 0.1)            # value vor top p sampling\n\u001b[0m\u001b[1;32m     16\u001b[0m                                                         \u001b[0;31m# at each step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-587e69c04c17>\u001b[0m in \u001b[0;36mcreate_aliterations\u001b[0;34m(prompt, letters, tokenizer, word_beginnings, max_length, num_beams, num_return_beams, ngram_size_words, ngram_size_tokens, max_word_len, len_rand, nucleus_sampling, top_p)\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                         \u001b[0mlen_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_rand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                         \u001b[0mnucleus_sampling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnucleus_sampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                         top_p = top_p)])\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     generated = model.beam_search(\n",
      "\u001b[0;32m<ipython-input-7-42e36341949d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, first_stress, word_beginnings, ngram_size_words, ngram_size_tokens, max_word_len, len_rand, nucleus_sampling, top_p, len_metrum, len_verse)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_stress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_stress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'notstressed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlst_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'notstressed'"
     ]
    }
   ],
   "source": [
    "prompt = '<|endoftext|>'\n",
    "\n",
    "letters = ['d','a']\n",
    "generated = create_aliterations(prompt,letters,tokenizer,\n",
    "                                word_beginnings = True, # only the words should begin with the same character,\n",
    "                                                        # if False all tokens will begin with the same character\n",
    "                                max_length = 15,        # number of tokens after which the beam search stops\n",
    "                                num_beams = 10,         # number of beams the algorithm will try\n",
    "                                num_return_beams = 5,  # total number of beams that will be kept after each step\n",
    "                                ngram_size_words = 2,   # maximum number a word n-gram may be repeated\n",
    "                                ngram_size_tokens = 4,  # maximum number of token n-gram may be repeaded\n",
    "                                max_word_len = 4,       # maximum number of tokens a word may contain\n",
    "                                len_rand = False,       # make the maximum number of tokens for a word random \n",
    "                                nucleus_sampling = False,# do additional top p sampling\n",
    "                                top_p = 0.1)            # value vor top p sampling\n",
    "                                                        # at each step\n",
    "\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized,skip_special_tokens = True)\n",
    "  print(f'beam {index}: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "rythm_df = pd.read_csv('word_rythm.csv')\n",
    "\n",
    "words = 'und sch√∂n ists zu scherzen und scherzen mit dir und mit den andern'.split()\n",
    "\n",
    "for word in words: \n",
    "    print(rythm_df.loc[(rythm_df['word'] == word)]['num_syll'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "print(rythm_df.loc[(rythm_df['word'] == 'sah')]['rythm'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[199]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[14,12]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LLM_alliterations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
