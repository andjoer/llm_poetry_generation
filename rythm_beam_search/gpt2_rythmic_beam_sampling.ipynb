{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling with rythmic constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('Anjoe/german-poetry-gpt2-large')\n",
    "model = GPT2LMHeadModel.from_pretrained('Anjoe/german-poetry-gpt2-large') \n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "The top p / top k filtered logits are considered to be a candidate. The candidates are checked with a dictionary if they could lead to the targeted rythm. If this is the case, they are stored in a list. For each entry of a list in a next round the process is repeated, while now the sequence of both tokens are checked in the dictionary. If the none of the predicted sequences is in a list, the branch will be closed and deleted.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_next(text,\n",
    "              target_dict,\n",
    "              target_dict_s,\n",
    "              vocabulary,\n",
    "              new_words,\n",
    "              next_stress,\n",
    "              rythm_df,\n",
    "              new_line,\n",
    "              top_p = 0.4,\n",
    "              max_top_k = 100,\n",
    "              min_first_branches = 10,\n",
    "              sample = True,\n",
    "              rand = 4):\n",
    "    \n",
    "    inputs = tokenizer(text,return_tensors='pt')['input_ids']\n",
    "    inputs_0 = inputs\n",
    "    word_lst_0 = text.split()\n",
    "    word_lst = word_lst_0\n",
    "    text_out = text\n",
    "    text_0 = text\n",
    "    \n",
    "    last_tokens = []\n",
    "    first = True\n",
    "    max_reset = 200\n",
    "    reset = 0\n",
    "    num_created_outs = 0\n",
    "    \n",
    "    hypothesis = []\n",
    "    depth = 0\n",
    "    first = True\n",
    "    while True:\n",
    "        word_lst = text_out.split()\n",
    "        \n",
    "        text = text_out\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        num_created_outs += 1\n",
    "\n",
    "        logits= outputs.logits[:,-1,:]\n",
    "\n",
    "        next_token_sample = torch.argsort(-logits)\n",
    "        \n",
    "        ################################\n",
    "        # top p Filtering\n",
    "        \n",
    "        m = torch.nn.Softmax(dim = 1)\n",
    "        probabilites_sm_sorted = torch.sort(m(logits),descending=True)\n",
    "        cum_prob = 0\n",
    "        top_k = 0\n",
    "        probabilities_sorted = probabilites_sm_sorted[0][0].cpu().detach().numpy()\n",
    "        \n",
    "        while cum_prob < top_p:\n",
    "            cum_prob += probabilities_sorted[top_k]\n",
    "            top_k += 1\n",
    "            \n",
    "        top_k += 1\n",
    "\n",
    "        ##################################    \n",
    "\n",
    "        candidates = next_token_sample[:,:top_k].cpu().detach().numpy()[0]\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        if first and not new_line:                           # word preceeded by space\n",
    "\n",
    "            for idx, cand in enumerate(list(candidates)):\n",
    "                \n",
    "                score = min(target_dict_s[cand],1)\n",
    "                scores.append(score)    \n",
    "            \n",
    "            \n",
    "        else:\n",
    "            for idx, cand in enumerate(list(candidates)):\n",
    "\n",
    "                score = min(target_dict[str(last_tokens+[cand])],1) # word consisting out of more tokens\n",
    "                if cand in new_words:                               # each predicted new word is ok\n",
    "                                                                    # since it only marks the end of the previous\n",
    "                    score = 1\n",
    "\n",
    "                scores.append(score)  \n",
    "        \n",
    "        first = False\n",
    "        \n",
    "        if len(scores) == 0 or (np.sum(scores) == 0 and depth > 0) or (np.sum(scores) < min_first_branches and depth == 0):  \n",
    "            k = top_k                                           # if no candidate is found within top p extend top k\n",
    "            \n",
    "            while k < (max_top_k/(depth*2+1)):\n",
    "                cand = next_token_sample[:,k].detach().numpy()[0]\n",
    "                scores.append(min(target_dict[str(last_tokens+[cand])],1))\n",
    "                k += 1\n",
    "                if np.sum(scores) > 0 and depth > 0:\n",
    "                    break\n",
    "                    \n",
    "                if np.sum(scores) >= min_first_branches and depth == 0:       # minimum initial branches\n",
    "                    break\n",
    "            top_k = k\n",
    "                    \n",
    "            candidates = next_token_sample[:,:top_k].cpu().detach().numpy()[0]\n",
    "  \n",
    "        cand_probs = probabilities_sorted[:top_k] \n",
    "        if sum(scores) > 0:\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            scores_sort = np.sort(-scores)\n",
    "\n",
    "            min_0 = np.max(np.nonzero(scores_sort))+1\n",
    "\n",
    "            \n",
    "            if sample:                  # sort the tokens according to their probabilities; \n",
    "                                        # some randomness is needed to prevent repetitions at line-reset\n",
    "                                                                \n",
    "                tokens_delete = np.argsort(-scores)[min_0:]\n",
    "                cand_probs[tokens_delete] = -float('inf')\n",
    "                cand_probs *= np.random.randn(len(cand_probs))/rand+1\n",
    "                scores_arg = np.argsort(-cand_probs)[:min_0]\n",
    "            else:\n",
    "                scores_arg = np.argsort(-scores)[:min_0]\n",
    "\n",
    "                random.shuffle(scores_arg)                # just shuffle random; no multinomial sampling\n",
    "\n",
    "         \n",
    "            candidates_fin = candidates[scores_arg]\n",
    "     \n",
    "            if depth == 0:\n",
    "                if 199 in candidates_fin:\n",
    "                    return 1, '\\n'\n",
    "                if candidates_fin[0] in [12,14]:\n",
    "                    return 1, tokenizer.decode(candidates_fin[0])\n",
    "\n",
    "            if depth > 0:\n",
    "                candidates_no_start = [cand for cand in candidates_fin if cand not in new_words+[199,12,14]]\n",
    "            else:\n",
    "                candidates_no_start = candidates_fin\n",
    "\n",
    "\n",
    "\n",
    "            if (len(candidates_fin) - len(candidates_no_start)) > 0:    #word could have an end\n",
    "\n",
    "                last_word = tokenizer.decode(last_tokens).lower().strip()\n",
    "                if last_word in vocabulary:\n",
    "                    start_stress = rythm_df.loc[(rythm_df['word'] == last_word)]['start'].values[0] \n",
    "\n",
    "                    if start_stress == next_stress or start_stress == 0.5:\n",
    "\n",
    "                        return num_created_outs, last_word\n",
    "        else:\n",
    "            candidates_no_start = []\n",
    "\n",
    "        if len(candidates_no_start) == 0: # close branch\n",
    "\n",
    "            for j in range(len(hypothesis)):\n",
    "                hypothesis[-1] = hypothesis[-1][:-1]\n",
    "                if len(hypothesis[-1]) < 1:\n",
    "                    hypothesis = hypothesis[:-1]\n",
    "                    depth -= 1\n",
    "                else: break\n",
    "\n",
    "            last_tokens = []\n",
    "            for token in hypothesis:\n",
    "                last_tokens.append(token[-1])\n",
    "            if not last_tokens:\n",
    "                return num_created_outs, False\n",
    "            inputs = torch.cat((inputs_0, torch.tensor([last_tokens])),1)\n",
    "\n",
    "        else:\n",
    "            depth += 1\n",
    "            hypothesis.append(candidates_no_start)  # each valid token is appended to the search tree\n",
    "            last_tokens = []\n",
    "            for token in hypothesis:\n",
    "                last_tokens.append(token[-1])\n",
    "            inputs = torch.cat((inputs_0, torch.tensor([last_tokens])),1)\n",
    "\n",
    "            text_out = tokenizer.decode(inputs[0])\n",
    "\n",
    "    \n",
    "    return reset, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'rythm_beam_search/notstressed', 'rb') as f:\n",
    "    lst_0 = pickle.load(f)\n",
    "    \n",
    "with open(r'rythm_beam_search/stressed', 'rb') as f:\n",
    "    lst_1 = pickle.load(f)\n",
    "    \n",
    "with open(r'rythm_beam_search/notstressed_start', 'rb') as f:\n",
    "    lst_0_s = pickle.load(f)\n",
    "    \n",
    "with open(r'rythm_beam_search/stressed_start', 'rb') as f:\n",
    "    lst_1_s = pickle.load(f)\n",
    "    \n",
    "rythm_df = pd.read_csv('rythm_beam_search/word_rythm.csv')\n",
    "\n",
    "two_ltr_words =['ab', 'am', 'an', 'da', 'du', 'eh', 'er', 'es', 'im', 'in', 'ja', 'je', 'ob', 'so', 'um',\n",
    "                   'wo', 'zu','ha','oh','ui']\n",
    "rythm_df['reject'] = rythm_df['word'].apply(lambda x: len(x) < 3 and x not in two_ltr_words if\n",
    "                                             type(x) == str else True)\n",
    "\n",
    "rythm_df = rythm_df.drop(rythm_df[rythm_df.reject == True].index)\n",
    "                                \n",
    "new_words = list(lst_1_s.keys()) + list(lst_0_s.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the poem\n",
    "\n",
    "The poem gets generated word by word. If the Algorithm is not able to finish a line, it gets resetted. At the end it is possible to check how many token generations had been necessary in order to create the four lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " leb frohlockend darum eile \n",
      "\n",
      "number of generated outputs:\n",
      "15\n",
      "number of resetted lines:\n",
      "0\n",
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " leb frohlockend darum eile \n",
      " wer geduld get raget tugendreichem \n",
      "\n",
      "number of generated outputs:\n",
      "57\n",
      "number of resetted lines:\n",
      "1\n",
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " leb frohlockend darum eile \n",
      " wer geduld get raget tugendreichem \n",
      " in vollendung seines hobbit . \n",
      "\n",
      "number of generated outputs:\n",
      "72\n",
      "number of resetted lines:\n",
      "1\n",
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " leb frohlockend darum eile \n",
      " wer geduld get raget tugendreichem \n",
      " in vollendung seines hobbit . \n",
      " leb , bewahr est recke werth \n",
      "\n",
      "number of generated outputs:\n",
      "86\n",
      "number of resetted lines:\n",
      "1\n",
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " leb frohlockend darum eile \n",
      " wer geduld get raget tugendreichem \n",
      " in vollendung seines hobbit . \n",
      " leb , bewahr est recke werth \n",
      "\n",
      "number of generated outputs:\n",
      "86\n",
      "number of resetted lines:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "text = '''Nur durch das Morgentor des Schönen\n",
    "Drangst du in der Erkenntnis Land.\n",
    "An höhern Glanz sich zu gewöhnen,\n",
    "Übt sich am Reize der Verstand.\n",
    "'''\n",
    "\n",
    "first_stress = 1\n",
    "\n",
    "len_metrum = 2\n",
    "len_verse = 10\n",
    "vocabulary = list(rythm_df['word'])\n",
    "new_line = True\n",
    "\n",
    "num_created_outs = 0\n",
    "resets_new_line = 0\n",
    "lines = text.split('\\n')\n",
    "\n",
    "lines_0 = len(lines)\n",
    "\n",
    "while len(lines) <= 8:  \n",
    "    \n",
    "    words = re.sub('[\\W_]+', ' ', lines[-1]).split()\n",
    "    num_syll = 0\n",
    "    if words:\n",
    "        for word in words:\n",
    "            num_syll += rythm_df.loc[(rythm_df['word'] == word.lower())]['num_syll'].values[0]\n",
    "\n",
    "\n",
    "    if num_syll % len_metrum == 0:\n",
    "        next_stress = first_stress     \n",
    "    else:\n",
    "        next_stress = (1-first_stress)**2\n",
    "                      \n",
    "    if next_stress == 1: \n",
    "        target_dict = lst_1\n",
    "        target_dict_s = lst_1_s\n",
    "    \n",
    "    else:\n",
    "        target_dict = lst_0\n",
    "        target_dict_s = lst_0_s\n",
    "        \n",
    "    target_dict_s[14] = 1     # .\n",
    "    target_dict_s[12] = 1     # ,\n",
    "\n",
    "    if num_syll > len_verse - 4:\n",
    "        target_dict_s[199] = 1  # \\n\n",
    "        \n",
    "    else:\n",
    "        target_dict_s[199] = 0\n",
    "\n",
    "    num_outs, next_word = pred_next(text,\n",
    "                          target_dict,\n",
    "                          target_dict_s,\n",
    "                          vocabulary,\n",
    "                          new_words,\n",
    "                          next_stress, \n",
    "                          rythm_df,\n",
    "                          new_line,\n",
    "                          top_p = 0.25,            # top p value\n",
    "                          max_top_k = 130,         # maximum to which the top k value will be extended\n",
    "                                                   # when there are too few branches\n",
    "                          min_first_branches = 10, # minimum of initial branches/beams\n",
    "                          sample = True,           # tokens with higher probability will be prefered\n",
    "                          rand = 5)                # randomize, larger value is less random\n",
    "\n",
    "    num_created_outs += num_outs\n",
    "    \n",
    "    if next_word == '\\n':\n",
    "        new_line = True\n",
    "    else:\n",
    "        new_line = False\n",
    "    \n",
    "    if next_word: \n",
    "        text += ' ' + next_word\n",
    "    else:\n",
    "        text = '\\n'.join(text.split('\\n')[:-1]) +'\\n'\n",
    "        resets_new_line += 1\n",
    "        new_line = True\n",
    "\n",
    "    lines = text.split('\\n')    \n",
    "    if len(lines) > lines_0:\n",
    "        lines_0 += 1\n",
    "        print('result')\n",
    "        print(text)\n",
    "        print('number of generated outputs:')\n",
    "        print(num_created_outs)\n",
    "        print('number of resetted lines:')\n",
    "        print(resets_new_line)\n",
    "\n",
    "print('result')\n",
    "print(text)\n",
    "print('number of generated outputs:')\n",
    "print(num_created_outs)\n",
    "print('number of resetted lines:')\n",
    "print(resets_new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm used in the poem generator\n",
    "Below the same experiment is done with the algorithm that is used in the poem generator. It randomly generates n sequences and checks if one of them fullfills the metrical constrains. Therefore a lot more token need to be generated. However the output is not constrained by metric dictionaries and the sampling can be done according to other criteria as well. Therefore the linguistic quality of the produced output is usually better.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/.conda/envs/tf36/lib/python3.6/site-packages/packaging/requirements.py:66: UserWarning: warn_ungrouped_named_tokens_in_collection: setting results name 'specifier' on And expression collides with '_original_end' on contained expression\n",
      "  VERSION_SPEC = originalTextFor(_VERSION_SPEC)(\"specifier\")\n",
      "/home/andreas/.conda/envs/tf36/lib/python3.6/site-packages/packaging/requirements.py:69: UserWarning: warn_ungrouped_named_tokens_in_collection: setting results name 'marker' on And expression collides with '_original_end' on contained expression\n",
      "  MARKER_EXPR = originalTextFor(MARKER_EXPR())(\"marker\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " Was das Reich der Geister brachte weiß\n",
      "\n",
      "number of generated outputs:\n",
      "400\n",
      "number of resetted lines:\n",
      "0\n",
      "start generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " Was das Reich der Geister brachte weiß\n",
      " ich Ich heute , aber ich durfte schaun !\n",
      "\n",
      "number of generated outputs:\n",
      "1100\n",
      "number of resetted lines:\n",
      "0\n",
      "start generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " Was das Reich der Geister brachte weiß\n",
      " ich Ich heute , aber ich durfte schaun !\n",
      " Als du noch in deinem Dome sangst ,\n",
      "\n",
      "number of generated outputs:\n",
      "2500\n",
      "number of resetted lines:\n",
      "0\n",
      "start generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " Was das Reich der Geister brachte weiß\n",
      " ich Ich heute , aber ich durfte schaun !\n",
      " Als du noch in deinem Dome sangst ,\n",
      " War es jetzt bist du und dämmernd steigt\n",
      "\n",
      "number of generated outputs:\n",
      "3000\n",
      "number of resetted lines:\n",
      "0\n",
      "start generating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result\n",
      "Nur durch das Morgentor des Schönen\n",
      "Drangst du in der Erkenntnis Land.\n",
      "An höhern Glanz sich zu gewöhnen,\n",
      "Übt sich am Reize der Verstand.\n",
      " Was das Reich der Geister brachte weiß\n",
      " ich Ich heute , aber ich durfte schaun !\n",
      " Als du noch in deinem Dome sangst ,\n",
      " War es jetzt bist du und dämmernd steigt\n",
      "\n",
      "number of generated outputs:\n",
      "4600\n",
      "number of resetted lines:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for the sampling refer to https://huggingface.co/transformers/v3.1.0/_modules/transformers/generation_utils.html\n",
    "if temperature != 1.0:\n",
    "    scores = scores / temperature\n",
    "# Top-p/top-k filtering\n",
    "next_token_logscores = top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\n",
    "# Sample\n",
    "probs = F.softmax(next_token_logscores, dim=-1)\n",
    "\n",
    "'''\n",
    "import include_dir\n",
    "\n",
    "from gpt_poet import gpt_poet_analysis\n",
    "\n",
    "text = '''Nur durch das Morgentor des Schönen\n",
    "Drangst du in der Erkenntnis Land.\n",
    "An höhern Glanz sich zu gewöhnen,\n",
    "Übt sich am Reize der Verstand.\n",
    "'''\n",
    "num_syll = 10\n",
    "target_rythm = [1,0]\n",
    "\n",
    "num_created_outs = 0\n",
    "resets_new_line = 0\n",
    "lines = text.split('\\n')\n",
    "while len(lines) <= 8:\n",
    "    lines = text.split('\\n')\n",
    "    num_outs, next_line = gpt_poet_analysis(text,\n",
    "                                            target_rythm,\n",
    "                                            num_syll,\n",
    "                                            tollerance = 4,\n",
    "                                            require_last = True,   # if the last stress of the output should match\n",
    "                                            num_branches = 5,   # number of branches to try per iteration\n",
    "                                            LLM = 'GPT2-large')\n",
    "    num_created_outs += num_outs\n",
    "    if next_line: \n",
    "        text += ' ' + next_line\n",
    "    else:\n",
    "        resets_new_line += 1\n",
    "    print('result')\n",
    "    print(text)\n",
    "    print('number of generated outputs:')\n",
    "    print(num_created_outs)\n",
    "    print('number of resetted lines:')\n",
    "    print(resets_new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
