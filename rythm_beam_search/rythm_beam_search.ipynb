{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83YQtxlRLHyw"
   },
   "source": [
    "# Beamsearch with rythmic constrains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyP62vGZLI9w"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yvxh2q6YLHy1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BeamSearchScorer, LogitsProcessorList, MaxLengthCriteria, StoppingCriteriaList\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pyphen\n",
    "hyp_dic = pyphen.Pyphen(lang='de_DE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SYp5zlU_LHy4"
   },
   "outputs": [],
   "source": [
    "model_name = \"Anjoe/german-poetry-gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jsCo6ZhOLHy5"
   },
   "outputs": [],
   "source": [
    "def create_word_start_mask(tokenizer):    \n",
    "    word_start_mask =  np.zeros(len(tokenizer))\n",
    "    for i in range(len(tokenizer)):\n",
    "        if tokenizer.decode(i)[0] == ' ':\n",
    "            word_start_mask[i] = 1\n",
    "    return word_start_mask\n",
    "        \n",
    "def perplexity(text):\n",
    "    device = model.device\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    max_length = model.config.n_positions\n",
    "    stride = 512\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(0, encodings.input_ids.size(1), stride):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    return torch.exp(torch.stack(nlls).sum() / end_loc).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bUaToIPFLHy7"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "# Modified so that it works more on a word level. \n",
    "# Example \"das Denkende\" and \"das Denken\" are the same n-gram. \n",
    "################################################################################\n",
    "\n",
    "def _get_word_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx]\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_word_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "\n",
    "    cur_len = len(prev_input_ids)\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len])\n",
    "\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_word_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, \n",
    "                                   num_hypos: int, cur_len: int, word_start_mask: np.array):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    prev_input_ids = [[item for item in prev_inputs.tolist() if word_start_mask[item]==1] \n",
    "                      for prev_inputs in prev_input_ids] # MODIFICATION\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_word_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_word_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "\n",
    "    return banned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sK4p6HwTLHy9"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Original: https://huggingface.co/transformers/v4.6.0/_modules/transformers/generation_logits_process.html     \n",
    "################################################################################\n",
    "\n",
    "def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):\n",
    "    generated_ngrams = [{} for _ in range(num_hypos)]\n",
    "    for idx in range(num_hypos):\n",
    "        gen_tokens = prev_input_ids[idx].tolist()\n",
    "        generated_ngram = generated_ngrams[idx]\n",
    "        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):\n",
    "            prev_ngram_tuple = tuple(ngram[:-1])\n",
    "            generated_ngram[prev_ngram_tuple] = generated_ngram.get(prev_ngram_tuple, []) + [ngram[-1]]\n",
    "    return generated_ngrams\n",
    "\n",
    "\n",
    "def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):\n",
    "    # Before decoding the next token, prevent decoding of ngrams that have already appeared\n",
    "    start_idx = cur_len + 1 - ngram_size\n",
    "    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())\n",
    "    return banned_ngrams.get(ngram_idx, [])\n",
    "\n",
    "\n",
    "def _calc_banned_ngram_tokens(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int):\n",
    "    \"\"\"Copied from fairseq for no_repeat_ngram in beam_search\"\"\"\n",
    "    if cur_len + 1 < ngram_size:\n",
    "        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n",
    "        return [[] for _ in range(num_hypos)]\n",
    "\n",
    "    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)\n",
    "\n",
    "    banned_tokens = [\n",
    "        _get_generated_ngrams(generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len)\n",
    "        for hypo_idx in range(num_hypos)\n",
    "    ]\n",
    "    return banned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9mhzTzA3LHy_"
   },
   "outputs": [],
   "source": [
    "def find_word_beginning(token_ids):    \n",
    "    for j in range(1,len(token_ids)):\n",
    "                possible_beginning = tokenizer.decode(token_ids[-j])\n",
    "                if possible_beginning[0] == ' ' and possible_beginning.strip():\n",
    "                    return possible_beginning.strip(),j\n",
    "                    \n",
    "    else:\n",
    "        return False,0\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "- tokens which could not result in a correct meter (according to a dictionary) will be blocked\n",
    "- all tokens that would result in too many repetitions will be blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZvKnn-fULHy_"
   },
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "import numpy as np\n",
    "\n",
    "class verse_logits(LogitsProcessor):\n",
    "    def __init__(self, tokenizer,\n",
    "              first_stress = 1,  \n",
    "              offset = 1,\n",
    "              ngram_size_words = 2,\n",
    "              ngram_size_tokens = 4,\n",
    "              max_word_len = 4,\n",
    "              len_rand = True,\n",
    "              randomize = True,\n",
    "              random = 4,\n",
    "              len_metrum = 2,\n",
    "              len_verse = 10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.ngram_size_words = ngram_size_words\n",
    "        self.ngram_size_tokens = ngram_size_tokens\n",
    "        self.word_start_mask = create_word_start_mask(tokenizer)\n",
    "        self.max_word_len = max_word_len\n",
    "        self.len_rand = len_rand\n",
    "        self.randomize = randomize\n",
    "        self.random = random\n",
    "        self.first_stress = first_stress\n",
    "        self.len_metrum = len_metrum\n",
    "        self.delim = tokenizer.encode('.')+ tokenizer.encode(',')\n",
    "        self.new_line = tokenizer.encode('\\n')[0]\n",
    "        self.len_verse = len_verse\n",
    "        self.first_stress = first_stress\n",
    "        self.start = offset\n",
    "        \n",
    "                                \n",
    "        with open(r'notstressed', 'rb') as f:\n",
    "            self.lst_0 = pickle.load(f)\n",
    "    \n",
    "        with open(r'stressed', 'rb') as f:\n",
    "            self.lst_1 = pickle.load(f)\n",
    "    \n",
    "        with open(r'notstressed_start', 'rb') as f:\n",
    "            self.lst_0_s = pickle.load(f)\n",
    "    \n",
    "        with open(r'stressed_start', 'rb') as f:\n",
    "            self.lst_1_s = pickle.load(f)\n",
    "            \n",
    "        with open(r'stressed_start', 'rb') as f:\n",
    "            self.lst_1_s = pickle.load(f)\n",
    "        self.rythm_df = pd.read_csv('word_rythm.csv')\n",
    "\n",
    "        \n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        banned_tokens = []\n",
    "    \n",
    "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):    \n",
    "\n",
    "            banned = []\n",
    "            last_word, word_len = find_word_beginning(beam_input_ids)\n",
    "            text = self.tokenizer.decode(beam_input_ids[self.start:])  # start behind end of text token\n",
    "\n",
    "            all_words = re.sub('[\\W_]+', ' ', text).split()\n",
    "            words = all_words[:-1] #stop before last word that is not final\n",
    "            num_syll = 0\n",
    "            if words:\n",
    "                for word in words:\n",
    "                    num_syll += len(hyp_dic.inserted(word, ' ').split())\n",
    "\n",
    "            if num_syll % self.len_metrum == 0:\n",
    "                next_stress = self.first_stress\n",
    "            else:\n",
    "                next_stress = (1-self.first_stress)**2\n",
    "                \n",
    "            \n",
    "            if next_stress == 1:\n",
    "                target_dict = self.lst_1\n",
    "            else:\n",
    "                target_dict = self.lst_0\n",
    "            \n",
    "            last_stress = 5\n",
    "            last_stress_val = []\n",
    "            first_stress = 5\n",
    "            try:\n",
    "                last_stress = self.rythm_df.loc[(self.rythm_df['word'] == all_words[-1])]['end'].values[0]\n",
    "                first_stress = self.rythm_df.loc[(self.rythm_df['word'] == all_words[-1])]['start'].values[0]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            for i in range(len(tokenizer)):\n",
    "                \n",
    "                probability = 0\n",
    "                poss_word = str(list(beam_input_ids[self.start:].cpu().detach().numpy())[-word_len:] + [i])\n",
    "             \n",
    "                probability = target_dict[poss_word]\n",
    "                \n",
    "                if last_stress == 0 and first_stress == next_stress:\n",
    "                    probability = self.lst_1_s[i]\n",
    "                 \n",
    "                elif last_stress == 1 and first_stress == next_stress:\n",
    "                    probability = self.lst_0_s[i]\n",
    "                    \n",
    "               \n",
    "                if i == self.new_line and num_syll >= self.len_verse -5 and first_stress == next_stress:\n",
    "                    probability = 1\n",
    "                    \n",
    "                if probability == 0 or i < 33:\n",
    "                    banned.append(i)\n",
    "\n",
    "            banned_tokens.append(banned)\n",
    "            \n",
    "\n",
    "        num_batch_hypotheses = scores.shape[0]\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        banned_word_tokens = _calc_banned_word_ngram_tokens(self.ngram_size_words,\n",
    "                                                            input_ids,\n",
    "                                                            num_batch_hypotheses,\n",
    "                                                            cur_len,\n",
    "                                                            self.word_start_mask)\n",
    "        \n",
    "        banned_token_tokens = _calc_banned_ngram_tokens(self.ngram_size_tokens, \n",
    "                                                        input_ids, \n",
    "                                                        num_batch_hypotheses, \n",
    "                                                        cur_len)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (len(banned_tokens)): \n",
    "            banned_tokens[i] += banned_word_tokens[i] + banned_token_tokens[i]\n",
    "\n",
    "        for i, banned_token in enumerate(banned_tokens):\n",
    "            scores[i, banned_token] = -float(\"inf\")\n",
    "            \n",
    "        ##############################################################\n",
    "        # randomize\n",
    "        if self.randomize:\n",
    "     \n",
    "            scores *= torch.randn(scores.shape)/self.random+1\n",
    "\n",
    "        ###############################################################\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p1DKBu6DLHzE"
   },
   "outputs": [],
   "source": [
    "def create_verse(prompt, tokenizer,\n",
    "                       offset = 1,\n",
    "                       max_length = 24,\n",
    "                       num_beams = 15,\n",
    "                       num_return_beams = 14,\n",
    "                       ngram_size_words = 2,\n",
    "                       ngram_size_tokens = 4,\n",
    "                       max_word_len = 4,\n",
    "                       len_rand = False,\n",
    "                       randomize = False,\n",
    "                       random = 4 \n",
    "                       ):\n",
    "\n",
    "    if num_beams < num_return_beams:\n",
    "        print('warning: setting number of return beams equal to number of beams')\n",
    "        num_return_beams = num_beams\n",
    "    prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "    prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "    beam_scorer = BeamSearchScorer(\n",
    "        batch_size = prompt_tokenized.shape[0],\n",
    "        num_beams = num_beams,\n",
    "        num_beam_hyps_to_keep = num_return_beams,\n",
    "        device=model.device\n",
    "    )\n",
    "\n",
    "    \n",
    "    logits_processor = LogitsProcessorList([verse_logits(tokenizer,\n",
    "                                                        offset = offset,\n",
    "                                                        ngram_size_words = ngram_size_words,\n",
    "                                                        ngram_size_tokens = ngram_size_tokens,\n",
    "                                                        max_word_len = max_word_len,\n",
    "                                                        len_rand = len_rand,\n",
    "                                                        randomize = randomize,\n",
    "                                                        random = random)])\n",
    "\n",
    "    generated = model.beam_search(\n",
    "        torch.cat([prompt_tokenized] * num_beams),\n",
    "        beam_scorer,\n",
    "        logits_processor = logits_processor,\n",
    "        stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    "    )\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1TKh3s-HLHzG",
    "outputId": "2d02f2c3-0329-4c1b-bba9-5e6b25cbe715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam 0: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "\n",
      "\n",
      "beam 1: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick herab \n",
      "\n",
      "\n",
      "beam 2: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      " halbe\n",
      "beam 3: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick herab \n",
      " und\n",
      "beam 4: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick herab \n",
      " Lim\n",
      "beam 0: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "\n",
      "\n",
      "beam 1: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden sind im dunkeln Todtenreich\n",
      "beam 2: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden sind im dunkeln Todtenb\n",
      "beam 3: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden sind im dunkeln Todtenkranz\n",
      "beam 4: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden sind im dunkeln Dämmert\n",
      "beam 0: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "beam 1: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge222222'\n",
      "beam 2: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge222222;\n",
      "beam 3: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge222222/\n",
      "beam 4: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge222222,\n",
      "beam 0: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "Jünglingsse2222221\n",
      "beam 1: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "Jünglingsse222222'\n",
      "beam 2: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "Jünglingsse222222;\n",
      "beam 3: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "Jünglingsse222222/\n",
      "beam 4: Nur durch das Morgentor des Schönen\n",
      "Winkt ein feuervoller Blick hinaus \n",
      "Heiter auferstanden ist dereinst \n",
      "Himmelsauge2222221\n",
      "Jünglingsse222222,\n"
     ]
    }
   ],
   "source": [
    "prompt = '''Nur durch das Morgentor des Schönen\n",
    "'''\n",
    "\n",
    "for i in range(4):\n",
    "    offset = len(tokenizer.encode(prompt))\n",
    "    len_0 = len(prompt.split('\\n'))\n",
    "    generated = create_verse(prompt,tokenizer,\n",
    "                                    offset = offset,\n",
    "                                    max_length = offset + 12,   # number of tokens after which the beam search stops\n",
    "                                    num_beams = 14,         # number of beams the algorithm will try\n",
    "                                    num_return_beams = 5,  # total number of beams that will be kept after each step\n",
    "                                    ngram_size_words = 2,   # maximum number a word n-gram may be repeated\n",
    "                                    ngram_size_tokens = 4,  # maximum number of token n-gram may be repeaded\n",
    "                                    max_word_len = 4,       # maximum number of tokens a word may contain\n",
    "                                    len_rand = False,       # make the maximum number of tokens for a word random \n",
    "                                    randomize = True,     # additional sampling of the output\n",
    "                                    random = 6)            # random value for the output (more is less random)\n",
    "\n",
    "\n",
    "    for index, output_tokenized in enumerate(generated):\n",
    "        output = tokenizer.decode(output_tokenized,skip_special_tokens = True)\n",
    "        print(f'beam {index}: {output}')\n",
    "    out = tokenizer.decode(generated[0],skip_special_tokens = True).split('\\n')[len_0-1:len_0][0]\n",
    " \n",
    "    prompt += out + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "LLM_alliterations.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
